"""
AI Agent Service - A proactive AI teammate for pair programming
Uses GPT-4o-mini to analyze conversations and provide helpful insights
"""

import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from openai import OpenAI, AsyncOpenAI
from dataclasses import dataclass
import threading
import time
import base64
import random
import concurrent.futures

@dataclass
class Message:
    id: str
    content: str
    username: str
    userId: str
    timestamp: str
    room: str
    isAutoGenerated: bool = False

@dataclass
class ConversationContext:
    messages: List[Message]
    room_id: str
    last_ai_response: Optional[datetime] = None
    code_context: str = ""
    programming_language: str = "python"
    problem_description: str = ""
    problem_title: str = ""
    
    # Simple 5-second idle timer tracking
    last_message_time: Optional[datetime] = None  # When the last message was received
    pending_intervention_task: Optional[Any] = None  # Asyncio task for pending intervention

class AIAgent:
    def __init__(self, socketio_instance):
        # Check if OpenAI API key is available
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("‚ö†Ô∏è  Warning: No OpenAI API key found. AI agent will be disabled.")
            print("   Set OPENAI_API_KEY in your .env file to enable AI responses.")
            self.client = None
        else:
            try:
                self.client = OpenAI(api_key=api_key)
                self.async_client = AsyncOpenAI(api_key=api_key)  # For streaming TTS
                print("‚úÖ AI Agent (CodeBot) initialized successfully!")
            except Exception as e:
                print(f"‚ùå Error initializing OpenAI client: {e}")
                print("   AI agent will be disabled.")
                self.client = None
                self.async_client = None
        
        self.socketio = socketio_instance
        self.conversation_history = {}  # room_id -> ConversationContext
        
        # Simple timing parameters
        self.response_cooldown = 15  # Minimum seconds between AI responses
        self.min_messages_before_response = 3  # Wait for at least 3 messages before responding
        self.max_context_messages = 10  # Keep last 10 messages for context
        
        # AI Agent identity and voice configuration
        self.agent_name = "CodeBot"
        self.agent_id = "ai_agent_codebot"
        self.voice_config = {
            "model": "tts-1",            # Use OpenAI's fast TTS model (tts-1 or tts-1-hd)
            "voice": "nova",             # Available: alloy, echo, fable, onyx, nova, shimmer
            "speed": 1.0                 # 0.25 to 4.0
        }
        
        # Set up persistent event loop for async operations
        self._setup_persistent_event_loop()

    def _setup_persistent_event_loop(self):
        """Set up a persistent event loop for async operations"""
        def run_event_loop():
            self.async_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.async_loop)
            print("üîÑ Persistent event loop started for AI Agent async operations")
            self.async_loop.run_forever()
        
        # Start event loop in background thread
        self.async_thread = threading.Thread(target=run_event_loop, daemon=True)
        self.async_thread.start()
        
        # Wait for loop to be ready
        time.sleep(0.1)
        print("‚úÖ Persistent event loop ready for 5-second timers")
    
    def _cancel_pending_intervention(self, context: ConversationContext, room_id: str, reason: str):
        """Safely cancel any pending intervention task with improved error handling"""
        if not context.pending_intervention_task:
            return
            
        try:
            task = context.pending_intervention_task
            
            # Handle asyncio.Task (running in persistent event loop)
            if hasattr(task, 'cancel') and hasattr(task, 'cancelled'):
                if not task.done():
                    task.cancel()
                    print(f"üö´ CANCELLED intervention task ({reason}) in room {room_id}")
                else:
                    print(f"üîç Intervention task already completed in room {room_id}")
            
            # Handle concurrent.futures.Future (from run_coroutine_threadsafe)
            elif hasattr(task, 'cancel') and hasattr(task, 'done'):
                if not task.done():
                    task.cancel()
                    print(f"üö´ CANCELLED intervention future ({reason}) in room {room_id}")
                else:
                    print(f"üîç Intervention future already completed in room {room_id}")
            
            else:
                print(f"‚ö†Ô∏è  Unknown task type for intervention in room {room_id}: {type(task)}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Error cancelling intervention task in room {room_id}: {e}")
            print(f"   Task type: {type(context.pending_intervention_task)}")
            print(f"   Reason: {reason}")
        finally:
            # Always clear the task reference
            context.pending_intervention_task = None
    
    def _schedule_idle_intervention(self, context: ConversationContext, room_id: str):
        """Schedule a 5-second idle intervention timer with improved error handling"""
        # Double-check that no timer is already running (race condition prevention)
        if context.pending_intervention_task:
            print(f"‚ö†Ô∏è  Timer already exists for room {room_id}, skipping new timer creation")
            return
        
        async def delayed_intervention_check():
            timer_id = f"timer_{room_id}_{int(datetime.now().timestamp())}"
            try:
                print(f"‚è±Ô∏è  Starting 5-second idle timer {timer_id} for room {room_id}")
                
                # Store start time for validation
                timer_start_time = datetime.now()
                
                # Wait 5 seconds
                await asyncio.sleep(5.0)
                
                print(f"‚è∞  5-second idle timer {timer_id} completed for room {room_id}")
                
                # Validate that we're still the active timer (not replaced by a newer one)
                current_context = self.conversation_history.get(room_id)
                if not current_context:
                    print(f"üîÑ Context for room {room_id} no longer exists, skipping intervention")
                    return
                
                # Check if conversation is still idle (more robust validation)
                if current_context.last_message_time:
                    time_since_last = (datetime.now() - current_context.last_message_time).total_seconds()
                    time_since_timer_start = (datetime.now() - timer_start_time).total_seconds()
                    
                    # Only proceed if:
                    # 1. At least 5 seconds since last message
                    # 2. Timer ran for approximately 5 seconds
                    # 3. No new messages arrived during timer execution
                    if time_since_last >= 5.0 and 4.5 <= time_since_timer_start <= 6.0:
                        print(f"‚úÖ 5-second idle period confirmed for room {room_id}")
                        print(f"   Time since last message: {time_since_last:.1f}s")
                        print(f"   Timer execution time: {time_since_timer_start:.1f}s")
                        
                        # Check if we should respond (only after confirmed idle period)
                        if self.should_respond(room_id):
                            print(f"ü§ñ AI will respond after 5-second idle period in room {room_id}")
                            # Generate and send response asynchronously
                            response = await self.generate_response(room_id)
                            
                            if response:
                                await self.send_ai_message_with_audio(room_id, response)
                        else:
                            print(f"üö´ No intervention needed after 5-second idle period for room {room_id}")
                    else:
                        print(f"üîÑ Timing validation failed for room {room_id}")
                        print(f"   Time since last message: {time_since_last:.1f}s (need ‚â•5.0)")
                        print(f"   Timer execution time: {time_since_timer_start:.1f}s (need 4.5-6.0)")
                        
            except asyncio.CancelledError:
                print(f"üö´ Intervention timer {timer_id} cancelled for room {room_id}")
                raise  # Re-raise to properly handle cancellation
            except Exception as e:
                print(f"‚ùå Error in delayed intervention check {timer_id}: {e}")
                import traceback
                print(f"   Traceback: {traceback.format_exc()}")
            finally:
                # Clean up timer reference if we're still the active timer
                try:
                    current_context = self.conversation_history.get(room_id)
                    if current_context and current_context.pending_intervention_task:
                        # Only clear if this is the same task (avoid clearing newer timers)
                        task = current_context.pending_intervention_task
                        if hasattr(task, 'done') and task.done():
                            current_context.pending_intervention_task = None
                            print(f"üßπ Cleaned up completed timer reference for room {room_id}")
                except Exception as cleanup_error:
                    print(f"‚ö†Ô∏è  Error during timer cleanup: {cleanup_error}")
        
        # Schedule the timer using persistent event loop (improved scheduling)
        try:
            if hasattr(self, 'async_loop') and self.async_loop.is_running():
                # Use persistent event loop for reliable timer execution
                future = asyncio.run_coroutine_threadsafe(delayed_intervention_check(), self.async_loop)
                context.pending_intervention_task = future
                print(f"üìÖ Scheduled 5-second timer on persistent event loop for room {room_id}")
            else:
                # Fallback: try to use current event loop
                try:
                    current_loop = asyncio.get_running_loop()
                    task = current_loop.create_task(delayed_intervention_check())
                    context.pending_intervention_task = task
                    print(f"üìÖ Scheduled 5-second timer on current event loop for room {room_id}")
                except RuntimeError:
                    # No event loop available, skip timer (warn but don't crash)
                    print(f"‚ö†Ô∏è  No event loop available for 5-second timer in room {room_id}")
                    print(f"   Timer functionality will be disabled for this message")
                    
        except Exception as e:
            print(f"‚ùå Failed to schedule intervention timer for room {room_id}: {e}")
            print(f"   5-second idle functionality will be disabled for this message")

        
    def add_message_to_context(self, message_data: Dict[str, Any]):
        """Add a new message to the conversation context with research-based tracking"""
        room_id = message_data.get('room')
        if not room_id:
            return
            
        # Skip AI's own messages
        if message_data.get('userId') == self.agent_id:
            return
            
        message = Message(
            id=message_data.get('id', ''),
            content=message_data.get('content', ''),
            username=message_data.get('username', 'Unknown'),
            userId=message_data.get('userId', ''),
            timestamp=message_data.get('timestamp', ''),
            room=room_id,
            isAutoGenerated=message_data.get('isAutoGenerated', False)
        )
        
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.messages.append(message)
        
        # Update last message time for 5-second idle timer
        context.last_message_time = datetime.now()
        
        # Cancel any pending intervention since user is active
        self._cancel_pending_intervention(context, room_id, "new message received")
        
        # Keep only recent messages
        if len(context.messages) > self.max_context_messages:
            context.messages = context.messages[-self.max_context_messages:]
            
    def update_code_context(self, room_id: str, code: str, language: str = "python"):
        """Update the current code context for a room"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.code_context = code
        context.programming_language = language
        
    def update_problem_context(self, room_id: str, problem_title: str, problem_description: str):
        """Update the current problem description for a room"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.problem_title = problem_title
        context.problem_description = problem_description

    def should_respond(self, room_id: str) -> bool:
        """Simple decision making for AI intervention after 5-second idle"""
        if room_id not in self.conversation_history:
            print(f"üö´ AI WILL NOT RESPOND: No conversation history for room {room_id}")
            return False
            
        context = self.conversation_history[room_id]

        # Check cooldown period
        if context.last_ai_response:
            time_since_last = datetime.now() - context.last_ai_response
            if time_since_last < timedelta(seconds=self.response_cooldown):
                print(f"üö´ AI WILL NOT RESPOND: Cooldown period ({time_since_last.total_seconds():.1f}s < {self.response_cooldown}s)")
                return False
                
        # Need minimum number of messages
        if len(context.messages) < self.min_messages_before_response:
            print(f"üö´ AI WILL NOT RESPOND: Not enough messages ({len(context.messages)} < {self.min_messages_before_response})")
            return False
        
        # Simple AI decision using centralized LLM
        should_intervene, intervention_message = self._centralized_ai_decision(context)
        
        if should_intervene:
            print(f"‚úÖ AI WILL RESPOND: Intervention decision made for room {room_id}")
            # Store the intervention message for generate_response to use
            context.pending_intervention_message = intervention_message
        else:
            print(f"üö´ AI WILL NOT RESPOND: AI decided not to intervene for room {room_id}")
        
        return should_intervene
        
    async def generate_response(self, room_id: str) -> Optional[str]:
        """Generate AI response using centralized decision"""
        if not self.client:
            print("‚ö†Ô∏è  Cannot generate AI response: OpenAI client not initialized")
            return None
            
        if room_id not in self.conversation_history:
            return None
            
        context = self.conversation_history[room_id]
        
        # Check if we have a pre-generated intervention message from centralized decision
        if hasattr(context, 'pending_intervention_message') and context.pending_intervention_message:
            intervention_message = context.pending_intervention_message
            
            print(f"‚úÖ USING CENTRALIZED INTERVENTION: {len(intervention_message.split())} words")
            print(f"   Preview: {intervention_message[:100]}{'...' if len(intervention_message) > 100 else ''}")
            
            # Update tracking
            context.last_ai_response = datetime.now()
            
            # Clear the pending message
            context.pending_intervention_message = None
            
            return intervention_message
        else:
            print("‚ö†Ô∏è  No centralized intervention message found")
            return None
    
    # Legacy research-based response generation removed - replaced by centralized LLM decision
            
    async def generate_speech(self, text: str) -> Optional[bytes]:
        """Generate speech audio from text using OpenAI TTS"""
        if not self.client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            response = self.client.audio.speech.create(
                model=self.voice_config["model"],
                voice=self.voice_config["voice"],
                input=text,
                speed=self.voice_config["speed"]
            )
            
            # Return the audio bytes
            return response.content
            
        except Exception as e:
            print(f"Error generating speech: {e}")
            return None

    async def generate_streaming_speech(self, text: str, room_id: str, message_id: str):
        """Generate streaming speech audio using OpenAI's streaming TTS API (following official docs)"""
        if not self.async_client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            # Signal start of streaming
            self.socketio.emit('ai_audio_stream_start', {
                'messageId': message_id,
                'room': room_id
            }, room=room_id, namespace='/ws')
            
            chunk_number = 0
            total_bytes_sent = 0
            
            print(f"üé§ Starting to stream audio for text: '{text[:50]}...'")
            
            # Use OpenAI's official streaming approach with AsyncOpenAI
            async with self.async_client.audio.speech.with_streaming_response.create(
                model=self.voice_config["model"],  # tts-1
                voice=self.voice_config["voice"],  # nova
                input=text,
                speed=self.voice_config["speed"],
                response_format="pcm"  # PCM for true real-time streaming
            ) as response:
                chunks_sent = []
                # Stream chunks directly as they arrive - TRUE real-time streaming
                async for chunk in response.iter_bytes(chunk_size=1024 * 2):  # 2KB chunks for PCM real-time
                    if chunk:
                        chunk_number += 1
                        total_bytes_sent += len(chunk)
                        chunk_base64 = base64.b64encode(chunk).decode('utf-8')
                        chunks_sent.append(chunk_number)
                        
                        print(f"üì¶ Streaming PCM chunk {chunk_number}: {len(chunk)} bytes (real-time)")
                        
                        # Send chunk immediately as it arrives from OpenAI
                        self.socketio.emit('ai_audio_chunk', {
                            'messageId': message_id,
                            'audioData': chunk_base64,
                            'chunkNumber': chunk_number,
                            'totalBytes': total_bytes_sent,
                            'room': room_id,
                            'isComplete': False,  # We don't know if this is the last chunk yet
                            'isRealtime': True,
                            'format': 'pcm'  # PCM format for true real-time streaming
                        }, room=room_id, namespace='/ws')
                        
                        # No artificial delay - stream as fast as data arrives from OpenAI

                
                # print(f"üèÅ Sending final chunk marker for chunk {final_chunk_number}")
                
                # Send a special "final chunk" marker
                if chunks_sent:
                    final_chunk_number = chunks_sent[-1]
                    print(f"üèÅ Sending final chunk marker for chunk {final_chunk_number}")
                    
                    # Send a special "final chunk" marker
                    self.socketio.emit('ai_audio_chunk', {
                        'messageId': message_id,
                        'audioData': '',  # Empty data
                        'chunkNumber': final_chunk_number,
                        'totalBytes': total_bytes_sent,
                        'room': room_id,
                        'isComplete': True,  # Mark as final
                        'isRealtime': True,
                        'format': 'pcm',
                        'isFinalMarker': True  # Special flag to indicate this is just a marker
                    }, room=room_id, namespace='/ws')

                # Signal completion - only that streaming is done, not that playback is done
                self.socketio.emit('ai_audio_complete', {
                    'messageId': message_id,
                    'room': room_id,
                    'totalChunks': chunk_number,
                    'totalBytes': total_bytes_sent,
                    'format': 'pcm'
                }, room=room_id, namespace='/ws')
                
                # DON'T send ai_audio_done here - let frontend determine when playback is actually finished
                
                print(f"‚úÖ Streamed {chunk_number} PCM chunks ({total_bytes_sent} bytes) in real-time for message {message_id}")
            return True
            
        except Exception as e:
            print(f"Error generating streaming speech: {e}")
            # Signal error
            self.socketio.emit('ai_audio_error', {
                'messageId': message_id,
                'room': room_id,
                'error': str(e)
            }, room=room_id, namespace='/ws')
            
            # CRITICAL: Even on error, signal that audio streaming is done
            self.socketio.emit('ai_audio_done', {
                'messageId': message_id,
                'room': room_id,
                'status': 'error'
            }, room=room_id, namespace='/ws')
            
            return None

    async def send_ai_message_with_audio(self, room_id: str, content: str):
        """Send an AI message to the chat room with streaming audio - optimized for speed"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': True,  # Will have streaming audio
            'isStreaming': True  # Indicate this is a streaming response
        }
        
        # Update last response time for cooldown tracking
        if room_id in self.conversation_history:
            context = self.conversation_history[room_id]
            context.last_ai_response = datetime.now()
            print(f"üîí AI RESPONSE: Tracking response time for cooldown in room {room_id}")
        
        # Send message immediately (don't wait for audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
        
        # Generate streaming audio in parallel (non-blocking)
        def generate_and_stream_audio():
            try:
                # Run the async audio streaming in a new event loop
                async def audio_task():
                    await self.generate_streaming_speech(content, room_id, message['id'])
                
                # Create and run new event loop for audio generation
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(audio_task())
                loop.close()
            except Exception as e:
                print(f"Error in streaming audio: {e}")
                # Fallback to simple notification that audio failed
                self.socketio.emit('ai_audio_error', {
                    'messageId': message['id'],
                    'room': room_id,
                    'error': 'Audio generation failed'
                }, room=room_id, namespace='/ws')
        
        # Start audio streaming in a separate thread
        threading.Thread(target=generate_and_stream_audio, daemon=True).start()
    
    def send_ai_message_text_only(self, room_id: str, content: str):
        """Send an AI message to the chat room without audio"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': False
        }
        
        # Update last response time for non-greeting messages
        if room_id in self.conversation_history:
            self.conversation_history[room_id].last_ai_response = datetime.now()
            
        # Emit the message to the room (no audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
    
    def send_ai_message(self, room_id: str, content: str):
        """Send an AI message to the chat room (optimized sync version)"""
        try:
            # Try to use existing event loop if available
            try:
                loop = asyncio.get_running_loop()
                asyncio.create_task(self.send_ai_message_with_audio(room_id, content))
                return
            except RuntimeError:
                # No running loop - use thread pool to avoid blocking
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.send_ai_message_with_audio(room_id, content))
                    return
        except Exception as e:
            # Fallback to simple text-only message
            self.send_ai_message_text_only(room_id, content)
        
    async def process_message(self, message_data: Dict[str, Any]):
        """Process a new message and potentially respond"""
        room_id = message_data.get('room')
        if not room_id:
            return
            
        # Add message to context (this will cancel any pending intervention)
        self.add_message_to_context(message_data)
        
        # Get context for intervention scheduling (after message is added)
        if room_id not in self.conversation_history:
            return
            
        context = self.conversation_history[room_id]
        
        # Start new 5-second idle timer (improved with race condition prevention)
        self._schedule_idle_intervention(context, room_id)
                
    def process_message_sync(self, message_data: Dict[str, Any]):
        """Synchronous wrapper for processing messages - uses persistent event loop"""
        try:
            if hasattr(self, 'async_loop') and self.async_loop.is_running():
                # Use persistent event loop to avoid destroying timer tasks
                future = asyncio.run_coroutine_threadsafe(
                    self.process_message(message_data), 
                    self.async_loop
                )
                # Don't wait for completion - let it run asynchronously
                print(f"üì§ Message processing scheduled on persistent event loop")
            else:
                # Fallback to creating new event loop if persistent one isn't available
                print("‚ö†Ô∏è  Persistent event loop not available, falling back to temporary loop")
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(self.process_message(message_data))
                loop.close()
                
        except Exception as e:
            print(f"Error processing message in AI agent: {e}")
            # Continue processing other messages
            
    def handle_code_update(self, room_id: str, code: str, language: str = "python"):
        """Handle code updates from the editor"""
        self.update_code_context(room_id, code, language)
        
    def handle_problem_update(self, room_id: str, problem_title: str, problem_description: str):
        """Handle problem description updates"""
        self.update_problem_context(room_id, problem_title, problem_description)
        
    def release_generation_lock(self, room_id: str, message_id: str = None):
        """Release AI generation lock when audio playback is complete (simplified for current architecture)"""
        if room_id in self.conversation_history:
            # In the current simplified architecture, we don't maintain a generation lock
            # The 5-second idle timer and cooldown period handle response timing
            print(f"üîì Audio playback complete for room {room_id} (message: {message_id})")
            # Could add any cleanup logic here if needed in the future
        else:
            print(f"‚ö†Ô∏è AI RESPONSE LOCK: No context found for room {room_id}")
    
    def join_room(self, room_id: str):
        """AI agent joins a room"""
        # Only send greeting if OpenAI client is available
        if not self.client:
            print(f"‚ö†Ô∏è  CodeBot cannot join room {room_id}: OpenAI client not initialized")
            return
            
        # Send a greeting message when joining
        greeting_messages = [
            # "Hi! I'm CodeBot, your AI pair programming assistant. I'll help with technical questions, encourage good planning, and facilitate your collaboration. Let's code together!",
            # "Hello! I'm here to support your pair programming session. I can provide hints when you're stuck, help with code review, and ensure both of you stay engaged. Ready to start?",
            "Welcome! I'm CodeBot, designed to enhance your pair programming experience. I'll offer technical guidance and help maintain productive collaboration."
        ]
        
        import random
        greeting = random.choice(greeting_messages)
        
        # Send greeting after a short delay (non-blocking)
        def send_greeting():
            time.sleep(2)  # Wait 2 seconds before greeting
            self.send_ai_message_text_only(room_id, greeting)
            
        # Start greeting in a separate thread to avoid blocking
        threading.Thread(target=send_greeting, daemon=True).start()

    def set_voice_config(self, voice: str = None, model: str = None, speed: float = None):
        """Update voice configuration for TTS"""
        if voice and voice in ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]:
            self.voice_config["voice"] = voice
        if model and model in ["tts-1", "tts-1-hd"]:
            self.voice_config["model"] = model
        if speed and 0.25 <= speed <= 4.0:
            self.voice_config["speed"] = speed

    # Legacy background monitoring removed - replaced by 5-second idle timer and centralized decision
    
    def _centralized_ai_decision(self, context: ConversationContext) -> tuple[bool, str]:
        """Simple centralized AI decision: Should intervene and what to say"""
        if not self.client:
            print("üö´ AI WILL NOT INTERVENE: No LLM client available")
            return False, ""
        
        # Get recent conversation context (last 5 messages)
        recent_conversation = ""
        for msg in context.messages[-5:]:
            recent_conversation += f"{msg.username}: {msg.content}\n"
        
        # Simple comprehensive prompt
        prompt = f"""You are CodeBot, an AI pair programming assistant. Should you help in this conversation?

                    Problem Context:
                    - Problem: {context.problem_title or "General coding"}
                    - Language: {context.programming_language}

                    Recent Conversation:
                    {recent_conversation}

                    Code Context:
                    {context.code_context[:300] if context.code_context else "No code visible"}

                    Decide if you should help:
                    - Are they asking questions or need help? ‚Üí YES
                    - Are they stuck or confused? ‚Üí YES  
                    - Are they discussing well and making progress? ‚Üí NO

                    Response format:
                    - If you should help: "YES|[helpful message in 10-70 words - KEEP IT SHORT unless complex explanation needed]"
                    - If they're doing fine: "NO"

                    IMPORTANT: Prioritize SHORT, concise messages (10-30 words). Only use longer responses when absolutely necessary for clarity.

                    Your response:"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are CodeBot, a helpful pair programming assistant. Only intervene when truly helpful."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100,
                temperature=0.7
            )
            
            llm_response = response.choices[0].message.content.strip()
            
            if llm_response.startswith("YES|"):
                # Parse the response: YES|MESSAGE
                parts = llm_response.split("|", 1)
                if len(parts) >= 2:
                    intervention_message = parts[1]
                    print(f"‚úÖ AI WILL INTERVENE: {intervention_message[:50]}...")
                    return True, intervention_message
                else:
                    print(f"‚ùå LLM response format error: {llm_response}")
                    return False, ""
            else:
                print(f"üö´ AI WILL NOT INTERVENE: Users are discussing well")
                return False, ""
                
        except Exception as e:
            print(f"‚ùå Error in AI decision: {e}")
            return False, ""
    
    # Legacy complex intervention methods removed - replaced by centralized LLM decision
    # Legacy research-based intervention methods removed - all replaced by centralized LLM decision

# Global AI agent instance
ai_agent = None

def init_ai_agent(socketio_instance):
    """Initialize the AI agent"""
    global ai_agent
    ai_agent = AIAgent(socketio_instance)
    return ai_agent

def get_ai_agent():
    """Get the global AI agent instance"""
    return ai_agent
