"""
AI Agent Service - A proactive AI teammate for pair programming
Uses GPT-4o-mini to analyze conversations and provide helpful insights
"""

import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from openai import OpenAI, AsyncOpenAI
from dataclasses import dataclass
import threading
import time
import base64
import random
import concurrent.futures

@dataclass
class Message:
    id: str
    content: str
    username: str
    userId: str
    timestamp: str
    room: str
    isAutoGenerated: bool = False

@dataclass
class ConversationContext:
    messages: List[Message]
    room_id: str
    last_ai_response: Optional[datetime] = None
    code_context: str = ""
    programming_language: str = "python"
    problem_description: str = ""
    problem_title: str = ""
    
    # Simple timestamp tracking
    last_message_time: Optional[datetime] = None  # When the last message was received

class AIAgent:
    def __init__(self, socketio_instance):
        # Check if OpenAI API key is available
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("‚ö†Ô∏è  Warning: No OpenAI API key found. AI agent will be disabled.")
            print("   Set OPENAI_API_KEY in your .env file to enable AI responses.")
            self.client = None
        else:
            try:
                self.client = OpenAI(api_key=api_key)
                self.async_client = AsyncOpenAI(api_key=api_key)  # For streaming TTS
                print("‚úÖ AI Agent (CodeBot) initialized successfully!")
            except Exception as e:
                print(f"‚ùå Error initializing OpenAI client: {e}")
                print("   AI agent will be disabled.")
                self.client = None
                self.async_client = None
        
        self.socketio = socketio_instance
        self.conversation_history = {}  # room_id -> ConversationContext
        
        # Simple timing parameters
        self.response_cooldown = 15  # Minimum seconds between AI responses
        self.min_messages_before_response = 3  # Wait for at least 3 messages before responding
        self.max_context_messages = 10  # Keep last 10 messages for context
        
        # AI Agent identity and voice configuration
        self.agent_name = "CodeBot"
        self.agent_id = "ai_agent_codebot"
        self.voice_config = {
            "model": "tts-1",            # Use OpenAI's fast TTS model (tts-1 or tts-1-hd)
            "voice": "nova",             # Available: alloy, echo, fable, onyx, nova, shimmer
            "speed": 1.0                 # 0.25 to 4.0
        }
        
        # Simple timer tracking (much more efficient)
        self.pending_timers = {}  # room_id -> threading.Timer

    def _cancel_pending_intervention(self, room_id: str, reason: str):
        """Cancel any pending timer for a room"""
        if room_id in self.pending_timers:
            timer = self.pending_timers[room_id]
            timer.cancel()
            del self.pending_timers[room_id]
            print(f"üö´ CANCELLED timer ({reason}) in room {room_id}")
    
    def _schedule_idle_intervention(self, room_id: str):
        """Schedule a 5-second idle intervention timer using threading.Timer"""
        # Cancel existing timer
        self._cancel_pending_intervention(room_id, "new timer scheduled")
        
        def timer_callback():
            """Handle timer completion after 5 seconds"""
            try:
                print(f"‚è∞ 5-second timer completed for room {room_id}")
                
                # Clean up timer reference
                self.pending_timers.pop(room_id, None)
                
                # Check if we should respond
                if self.should_respond(room_id):
                    print(f"ü§ñ AI will respond after 5-second idle period in room {room_id}")
                    response = self._generate_response_sync(room_id)
                    if response:
                        self.send_ai_message(room_id, response)
                else:
                    print(f"üö´ No intervention needed after 5-second idle period for room {room_id}")
                        
            except Exception as e:
                print(f"‚ùå Timer callback error for room {room_id}: {e}")
        
        # Create and start timer
        timer = threading.Timer(5.0, timer_callback)
        timer.daemon = True
        timer.start()
        
        # Store timer reference
        self.pending_timers[room_id] = timer
        print(f"‚è±Ô∏è Started 5-second timer for room {room_id}")
    
    def _generate_response_sync(self, room_id: str) -> Optional[str]:
        """Synchronous response generation for timer callbacks"""
        if not self.client or room_id not in self.conversation_history:
            return None
            
        context = self.conversation_history[room_id]
        should_intervene, message = self._centralized_ai_decision(context)
        
        if should_intervene:
            context.last_ai_response = datetime.now()
            print(f"‚úÖ Generated response: {message[:50]}...")
            return message
        return None

        
    def add_message_to_context(self, message_data: Dict[str, Any]):
        """Add a new message to the conversation context with research-based tracking"""
        room_id = message_data.get('room')
        if not room_id:
            return
            
        # Skip AI's own messages
        if message_data.get('userId') == self.agent_id:
            return
            
        message = Message(
            id=message_data.get('id', ''),
            content=message_data.get('content', ''),
            username=message_data.get('username', 'Unknown'),
            userId=message_data.get('userId', ''),
            timestamp=message_data.get('timestamp', ''),
            room=room_id,
            isAutoGenerated=message_data.get('isAutoGenerated', False)
        )
        
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.messages.append(message)
        
        # Update last message time for 5-second idle timer
        context.last_message_time = datetime.now()
        
        # Cancel any pending intervention since user is active
        self._cancel_pending_intervention(room_id, "new message received")
        
        # Keep only recent messages
        if len(context.messages) > self.max_context_messages:
            context.messages = context.messages[-self.max_context_messages:]
            
    def update_code_context(self, room_id: str, code: str, language: str = "python"):
        """Update the current code context for a room"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.code_context = code
        context.programming_language = language
        
    def update_problem_context(self, room_id: str, problem_title: str, problem_description: str):
        """Update the current problem description for a room"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.problem_title = problem_title
        context.problem_description = problem_description

    def should_respond(self, room_id: str) -> bool:
        """Simple decision making for AI intervention after 5-second idle"""
        if room_id not in self.conversation_history:
            print(f"üö´ AI WILL NOT RESPOND: No conversation history for room {room_id}")
            return False
            
        context = self.conversation_history[room_id]

        # Check cooldown period
        if context.last_ai_response:
            time_since_last = datetime.now() - context.last_ai_response
            if time_since_last < timedelta(seconds=self.response_cooldown):
                print(f"üö´ AI WILL NOT RESPOND: Cooldown period ({time_since_last.total_seconds():.1f}s < {self.response_cooldown}s)")
                return False
                
        # Need minimum number of messages
        if len(context.messages) < self.min_messages_before_response:
            print(f"üö´ AI WILL NOT RESPOND: Not enough messages ({len(context.messages)} < {self.min_messages_before_response})")
            return False
        
        # Simple AI decision using centralized LLM
        should_intervene, intervention_message = self._centralized_ai_decision(context)
        
        if should_intervene:
            print(f"‚úÖ AI WILL RESPOND: Intervention decision made for room {room_id}")
            # Store the intervention message for generate_response to use
            context.pending_intervention_message = intervention_message
        else:
            print(f"üö´ AI WILL NOT RESPOND: AI decided not to intervene for room {room_id}")
        
        return should_intervene
        
    async def generate_response(self, room_id: str) -> Optional[str]:
        """Generate AI response using centralized decision"""
        if not self.client:
            print("‚ö†Ô∏è  Cannot generate AI response: OpenAI client not initialized")
            return None
            
        if room_id not in self.conversation_history:
            return None
            
        context = self.conversation_history[room_id]
        
        # Check if we have a pre-generated intervention message from centralized decision
        if hasattr(context, 'pending_intervention_message') and context.pending_intervention_message:
            intervention_message = context.pending_intervention_message
            
            print(f"‚úÖ USING CENTRALIZED INTERVENTION: {len(intervention_message.split())} words")
            print(f"   Preview: {intervention_message[:100]}{'...' if len(intervention_message) > 100 else ''}")
            
            # Update tracking
            context.last_ai_response = datetime.now()
            
            # Clear the pending message
            context.pending_intervention_message = None
            
            return intervention_message
        else:
            print("‚ö†Ô∏è  No centralized intervention message found")
            return None
    
    # Legacy research-based response generation removed - replaced by centralized LLM decision
            
    async def generate_speech(self, text: str) -> Optional[bytes]:
        """Generate speech audio from text using OpenAI TTS"""
        if not self.client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            response = self.client.audio.speech.create(
                model=self.voice_config["model"],
                voice=self.voice_config["voice"],
                input=text,
                speed=self.voice_config["speed"]
            )
            
            # Return the audio bytes
            return response.content
            
        except Exception as e:
            print(f"Error generating speech: {e}")
            return None

    async def generate_streaming_speech(self, text: str, room_id: str, message_id: str):
        """Generate streaming speech audio using OpenAI's streaming TTS API (following official docs)"""
        if not self.async_client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            # Signal start of streaming
            self.socketio.emit('ai_audio_stream_start', {
                'messageId': message_id,
                'room': room_id
            }, room=room_id, namespace='/ws')
            
            chunk_number = 0
            total_bytes_sent = 0
            
            print(f"üé§ Starting to stream audio for text: '{text[:50]}...'")
            
            # Use OpenAI's official streaming approach with AsyncOpenAI
            async with self.async_client.audio.speech.with_streaming_response.create(
                model=self.voice_config["model"],  # tts-1
                voice=self.voice_config["voice"],  # nova
                input=text,
                speed=self.voice_config["speed"],
                response_format="pcm"  # PCM for true real-time streaming
            ) as response:
                chunks_sent = []
                # Stream chunks directly as they arrive - TRUE real-time streaming
                async for chunk in response.iter_bytes(chunk_size=1024 * 2):  # 2KB chunks for PCM real-time
                    if chunk:
                        chunk_number += 1
                        total_bytes_sent += len(chunk)
                        chunk_base64 = base64.b64encode(chunk).decode('utf-8')
                        chunks_sent.append(chunk_number)
                        
                        print(f"üì¶ Streaming PCM chunk {chunk_number}: {len(chunk)} bytes (real-time)")
                        
                        # Send chunk immediately as it arrives from OpenAI
                        self.socketio.emit('ai_audio_chunk', {
                            'messageId': message_id,
                            'audioData': chunk_base64,
                            'chunkNumber': chunk_number,
                            'totalBytes': total_bytes_sent,
                            'room': room_id,
                            'isComplete': False,  # We don't know if this is the last chunk yet
                            'isRealtime': True,
                            'format': 'pcm'  # PCM format for true real-time streaming
                        }, room=room_id, namespace='/ws')
                        
                        # No artificial delay - stream as fast as data arrives from OpenAI

                
                # print(f"üèÅ Sending final chunk marker for chunk {final_chunk_number}")
                
                # Send a special "final chunk" marker
                if chunks_sent:
                    final_chunk_number = chunks_sent[-1]
                    print(f"üèÅ Sending final chunk marker for chunk {final_chunk_number}")
                    
                    # Send a special "final chunk" marker
                    self.socketio.emit('ai_audio_chunk', {
                        'messageId': message_id,
                        'audioData': '',  # Empty data
                        'chunkNumber': final_chunk_number,
                        'totalBytes': total_bytes_sent,
                        'room': room_id,
                        'isComplete': True,  # Mark as final
                        'isRealtime': True,
                        'format': 'pcm',
                        'isFinalMarker': True  # Special flag to indicate this is just a marker
                    }, room=room_id, namespace='/ws')

                # Signal completion - only that streaming is done, not that playback is done
                self.socketio.emit('ai_audio_complete', {
                    'messageId': message_id,
                    'room': room_id,
                    'totalChunks': chunk_number,
                    'totalBytes': total_bytes_sent,
                    'format': 'pcm'
                }, room=room_id, namespace='/ws')
                
                # DON'T send ai_audio_done here - let frontend determine when playback is actually finished
                
                print(f"‚úÖ Streamed {chunk_number} PCM chunks ({total_bytes_sent} bytes) in real-time for message {message_id}")
            return True
            
        except Exception as e:
            print(f"Error generating streaming speech: {e}")
            # Signal error
            self.socketio.emit('ai_audio_error', {
                'messageId': message_id,
                'room': room_id,
                'error': str(e)
            }, room=room_id, namespace='/ws')
            
            # CRITICAL: Even on error, signal that audio streaming is done
            self.socketio.emit('ai_audio_done', {
                'messageId': message_id,
                'room': room_id,
                'status': 'error'
            }, room=room_id, namespace='/ws')
            
            return None

    async def send_ai_message_with_audio(self, room_id: str, content: str):
        """Send an AI message to the chat room with streaming audio - optimized for speed"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': True,  # Will have streaming audio
            'isStreaming': True  # Indicate this is a streaming response
        }
        
        # Update last response time for cooldown tracking
        if room_id in self.conversation_history:
            context = self.conversation_history[room_id]
            context.last_ai_response = datetime.now()
            print(f"üîí AI RESPONSE: Tracking response time for cooldown in room {room_id}")
        
        # Send message immediately (don't wait for audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
        
        # Generate streaming audio in parallel (non-blocking)
        def generate_and_stream_audio():
            try:
                # Run the async audio streaming in a new event loop
                async def audio_task():
                    await self.generate_streaming_speech(content, room_id, message['id'])
                
                # Create and run new event loop for audio generation
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(audio_task())
                loop.close()
            except Exception as e:
                print(f"Error in streaming audio: {e}")
                # Fallback to simple notification that audio failed
                self.socketio.emit('ai_audio_error', {
                    'messageId': message['id'],
                    'room': room_id,
                    'error': 'Audio generation failed'
                }, room=room_id, namespace='/ws')
        
        # Start audio streaming in a separate thread
        threading.Thread(target=generate_and_stream_audio, daemon=True).start()
    
    def send_ai_message_text_only(self, room_id: str, content: str):
        """Send an AI message to the chat room without audio"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': False
        }
        
        # Update last response time for non-greeting messages
        if room_id in self.conversation_history:
            self.conversation_history[room_id].last_ai_response = datetime.now()
            
        # Emit the message to the room (no audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
    
    def send_ai_message(self, room_id: str, content: str):
        """Send an AI message to the chat room (optimized sync version)"""
        try:
            # Try to use existing event loop if available
            try:
                loop = asyncio.get_running_loop()
                asyncio.create_task(self.send_ai_message_with_audio(room_id, content))
                return
            except RuntimeError:
                # No running loop - use thread pool to avoid blocking
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.send_ai_message_with_audio(room_id, content))
                    return
        except Exception as e:
            # Fallback to simple text-only message
            self.send_ai_message_text_only(room_id, content)
        
    def process_message_sync(self, message_data: Dict[str, Any]):
        """Process a new message and potentially respond"""
        try:
            # Add message to context (this will cancel any pending intervention)
            self.add_message_to_context(message_data)
            
            # Get room_id and start new timer
            room_id = message_data.get('room')
            if room_id and room_id in self.conversation_history:
                # Start new 5-second idle timer
                self._schedule_idle_intervention(room_id)
                
        except Exception as e:
            print(f"Error processing message in AI agent: {e}")
            # Continue processing other messages
            
    def handle_code_update(self, room_id: str, code: str, language: str = "python"):
        """Handle code updates from the editor"""
        self.update_code_context(room_id, code, language)
        
    def handle_problem_update(self, room_id: str, problem_title: str, problem_description: str):
        """Handle problem description updates"""
        self.update_problem_context(room_id, problem_title, problem_description)
        
    def release_generation_lock(self, room_id: str, message_id: str = None):
        """Release AI generation lock when audio playback is complete (simplified for current architecture)"""
        if room_id in self.conversation_history:
            # In the current simplified architecture, we don't maintain a generation lock
            # The 5-second idle timer and cooldown period handle response timing
            print(f"üîì Audio playback complete for room {room_id} (message: {message_id})")
            # Could add any cleanup logic here if needed in the future
        else:
            print(f"‚ö†Ô∏è AI RESPONSE LOCK: No context found for room {room_id}")
    
    def join_room(self, room_id: str):
        """AI agent joins a room"""
        # Only send greeting if OpenAI client is available
        if not self.client:
            print(f"‚ö†Ô∏è  CodeBot cannot join room {room_id}: OpenAI client not initialized")
            return
            
        # Send a greeting message when joining
        greeting_messages = [
            # "Hi! I'm CodeBot, your AI pair programming assistant. I'll help with technical questions, encourage good planning, and facilitate your collaboration. Let's code together!",
            # "Hello! I'm here to support your pair programming session. I can provide hints when you're stuck, help with code review, and ensure both of you stay engaged. Ready to start?",
            "Welcome! I'm CodeBot, designed to enhance your pair programming experience. I'll offer technical guidance and help maintain productive collaboration."
        ]
        
        import random
        greeting = random.choice(greeting_messages)
        
        # Send greeting after a short delay (non-blocking)
        def send_greeting():
            time.sleep(2)  # Wait 2 seconds before greeting
            self.send_ai_message_text_only(room_id, greeting)
            
        # Start greeting in a separate thread to avoid blocking
        threading.Thread(target=send_greeting, daemon=True).start()

    def set_voice_config(self, voice: str = None, model: str = None, speed: float = None):
        """Update voice configuration for TTS"""
        if voice and voice in ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]:
            self.voice_config["voice"] = voice
        if model and model in ["tts-1", "tts-1-hd"]:
            self.voice_config["model"] = model
        if speed and 0.25 <= speed <= 4.0:
            self.voice_config["speed"] = speed

    # Legacy background monitoring removed - replaced by 5-second idle timer and centralized decision
    
    def _centralized_ai_decision(self, context: ConversationContext) -> tuple[bool, str]:
        """Simple centralized AI decision: Should intervene and what to say"""
        if not self.client:
            print("üö´ AI WILL NOT INTERVENE: No LLM client available")
            return False, ""
        
        # Get recent conversation context (last 5 messages)
        recent_conversation = ""
        for msg in context.messages[-5:]:
            recent_conversation += f"{msg.username}: {msg.content}\n"
        
        # Simple comprehensive prompt
        prompt = f"""You are CodeBot, an AI pair programming assistant. Should you help in this conversation?

                    Problem Context:
                    - Problem: {context.problem_title or "General coding"}
                    - Language: {context.programming_language}

                    Recent Conversation:
                    {recent_conversation}

                    Code Context:
                    {context.code_context[:300] if context.code_context else "No code visible"}

                    Decide if you should help:
                    - Are they asking questions or need help? ‚Üí YES
                    - Are they stuck or confused? ‚Üí YES  
                    - Are they discussing well and making progress? ‚Üí NO

                    Response format:
                    - If you should help: "YES|[helpful message in 10-70 words - KEEP IT SHORT unless complex explanation needed]"
                    - If they're doing fine: "NO"

                    IMPORTANT: Prioritize SHORT, concise messages (10-30 words). Only use longer responses when absolutely necessary for clarity.

                    Your response:"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are CodeBot, a helpful pair programming assistant. Only intervene when truly helpful."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100,
                temperature=0.7
            )
            
            llm_response = response.choices[0].message.content.strip()
            
            if llm_response.startswith("YES|"):
                # Parse the response: YES|MESSAGE
                parts = llm_response.split("|", 1)
                if len(parts) >= 2:
                    intervention_message = parts[1]
                    print(f"‚úÖ AI WILL INTERVENE: {intervention_message[:50]}...")
                    return True, intervention_message
                else:
                    print(f"‚ùå LLM response format error: {llm_response}")
                    return False, ""
            else:
                print(f"üö´ AI WILL NOT INTERVENE: Users are discussing well")
                return False, ""
                
        except Exception as e:
            print(f"‚ùå Error in AI decision: {e}")
            return False, ""
    
    # Legacy complex intervention methods removed - replaced by centralized LLM decision
    # Legacy research-based intervention methods removed - all replaced by centralized LLM decision

# Global AI agent instance
ai_agent = None

def init_ai_agent(socketio_instance):
    """Initialize the AI agent"""
    global ai_agent
    ai_agent = AIAgent(socketio_instance)
    return ai_agent

def get_ai_agent():
    """Get the global AI agent instance"""
    return ai_agent
