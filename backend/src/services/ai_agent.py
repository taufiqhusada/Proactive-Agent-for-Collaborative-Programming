"""
AI Agent Service - A proactive AI teammate for pair programming
Uses GPT-4o-mini to analyze conversations and provide helpful insights
"""

import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from openai import OpenAI, AsyncOpenAI
from dataclasses import dataclass
import threading
import time
import base64
import random
import concurrent.futures

@dataclass
class Message:
    id: str
    content: str
    username: str
    userId: str
    timestamp: str
    room: str
    isAutoGenerated: bool = False

@dataclass
class ConversationContext:
    messages: List[Message]
    room_id: str
    last_ai_response: Optional[datetime] = None
    code_context: str = ""
    programming_language: str = "python"
    problem_description: str = ""
    problem_title: str = ""
    
    # Simple timestamp tracking
    last_message_time: Optional[datetime] = None  # When the last message was received

class AIAgent:
    def __init__(self, socketio_instance):
        # Check if OpenAI API key is available
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("‚ö†Ô∏è  Warning: No OpenAI API key found. AI agent will be disabled.")
            print("   Set OPENAI_API_KEY in your .env file to enable AI responses.")
            self.client = None
        else:
            try:
                self.client = OpenAI(api_key=api_key)
                self.async_client = AsyncOpenAI(api_key=api_key)  # For streaming TTS
                print("‚úÖ AI Agent (CodeBot) initialized successfully!")
            except Exception as e:
                print(f"‚ùå Error initializing OpenAI client: {e}")
                print("   AI agent will be disabled.")
                self.client = None
                self.async_client = None
        
        self.socketio = socketio_instance
        self.conversation_history = {}  # room_id -> ConversationContext
        
        # Simple timing parameters
        self.response_cooldown = 15  # Minimum seconds between AI responses
        self.min_messages_before_response = 3  # Wait for at least 3 messages before responding
        self.max_context_messages = 10  # Keep last 10 messages for context
        
        # AI Agent identity and voice configuration
        self.agent_name = "CodeBot"
        self.agent_id = "ai_agent_codebot"
        self.voice_config = {
            "model": "tts-1",            # Use OpenAI's fast TTS model (tts-1 or tts-1-hd)
            "voice": "nova",             # Available: alloy, echo, fable, onyx, nova, shimmer
            "speed": 1.0                 # 0.25 to 4.0
        }
        
        # Simple timer tracking (much more efficient)
        self.pending_timers = {}  # room_id -> threading.Timer

    def _cancel_pending_intervention(self, room_id: str, reason: str):
        """Cancel any pending timer for a room"""
        if room_id in self.pending_timers:
            timer = self.pending_timers[room_id]
            timer.cancel()
            del self.pending_timers[room_id]
            print(f"üö´ CANCELLED timer ({reason}) in room {room_id}")
    
    def _schedule_idle_intervention(self, room_id: str):
        """Schedule a 5-second idle intervention timer using threading.Timer"""
        # Cancel existing timer
        self._cancel_pending_intervention(room_id, "new timer scheduled")
        
        def timer_callback():
            """Handle timer completion after 5 seconds"""
            try:
                print(f"‚è∞ 5-second timer completed for room {room_id}")
                
                # Clean up timer reference
                self.pending_timers.pop(room_id, None)
                
                # Check if we should respond
                if self.should_respond(room_id):
                    print(f"ü§ñ AI will respond after 5-second idle period in room {room_id}")
                    response = self._generate_response_sync(room_id)
                    if response:
                        self.send_ai_message(room_id, response)
                else:
                    print(f"üö´ No intervention needed after 5-second idle period for room {room_id}")
                        
            except Exception as e:
                print(f"‚ùå Timer callback error for room {room_id}: {e}")
        
        # Create and start timer
        timer = threading.Timer(5.0, timer_callback)
        timer.daemon = True
        timer.start()
        
        # Store timer reference
        self.pending_timers[room_id] = timer
        print(f"‚è±Ô∏è Started 5-second timer for room {room_id}")
    
    def _generate_response_sync(self, room_id: str) -> Optional[str]:
        """Synchronous response generation for timer callbacks"""
        if not self.client or room_id not in self.conversation_history:
            return None
            
        context = self.conversation_history[room_id]
        should_intervene, message = self._centralized_ai_decision(context)
        
        if should_intervene:
            context.last_ai_response = datetime.now()
            print(f"‚úÖ Generated response: {message[:50]}...")
            return message
        return None

        
    def add_message_to_context(self, message_data: Dict[str, Any]):
        """Add a new message to the conversation context with direct AI mention detection"""
        room_id = message_data.get('room')
        if not room_id:
            return
            
        # Skip AI's own messages
        if message_data.get('userId') == self.agent_id:
            return
            
        message = Message(
            id=message_data.get('id', ''),
            content=message_data.get('content', ''),
            username=message_data.get('username', 'Unknown'),
            userId=message_data.get('userId', ''),
            timestamp=message_data.get('timestamp', ''),
            room=room_id,
            isAutoGenerated=message_data.get('isAutoGenerated', False)
        )
        
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.messages.append(message)
        
        # Update last message time for 5-second idle timer
        context.last_message_time = datetime.now()
        
        # Cancel any pending intervention since user is active
        self._cancel_pending_intervention(room_id, "new message received")
        
        # Check for direct AI mention (@AI keyword) - PRIORITY RESPONSE
        if self._is_direct_ai_mention(message.content):
            print(f"üéØ DIRECT AI MENTION detected in room {room_id}: {message.content[:50]}...")
            # Respond immediately without waiting for 5-second timer
            self._handle_direct_ai_mention(room_id)
            # DO NOT start timer for direct mentions - return early
            if len(context.messages) > self.max_context_messages:
                context.messages = context.messages[-self.max_context_messages:]
            return
        
        # Keep only recent messages
        if len(context.messages) > self.max_context_messages:
            context.messages = context.messages[-self.max_context_messages:]
            
    def update_code_context(self, room_id: str, code: str, language: str = "python"):
        """Update the current code context for a room"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.code_context = code
        context.programming_language = language
        
        # Cancel any pending intervention since user is actively coding
        self._cancel_pending_intervention(room_id, "code update received")
        print(f"üñ•Ô∏è Code updated in room {room_id} - cancelled pending timer")
        
    def update_problem_context(self, room_id: str, problem_title: str, problem_description: str):
        """Update the current problem description for a room"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.problem_title = problem_title
        context.problem_description = problem_description

    def should_respond(self, room_id: str) -> bool:
        """Simple decision making for AI intervention after 5-second idle"""
        if room_id not in self.conversation_history:
            print(f"üö´ AI WILL NOT RESPOND: No conversation history for room {room_id}")
            return False
            
        context = self.conversation_history[room_id]

        # Check cooldown period
        if context.last_ai_response:
            time_since_last = datetime.now() - context.last_ai_response
            if time_since_last < timedelta(seconds=self.response_cooldown):
                print(f"üö´ AI WILL NOT RESPOND: Cooldown period ({time_since_last.total_seconds():.1f}s < {self.response_cooldown}s)")
                return False
                
        # Need minimum number of messages
        if len(context.messages) < self.min_messages_before_response:
            print(f"üö´ AI WILL NOT RESPOND: Not enough messages ({len(context.messages)} < {self.min_messages_before_response})")
            return False
        
        # Simple AI decision using centralized LLM
        should_intervene, intervention_message = self._centralized_ai_decision(context)
        
        if should_intervene:
            print(f"‚úÖ AI WILL RESPOND: Intervention decision made for room {room_id}")
            # Store the intervention message for generate_response to use
            context.pending_intervention_message = intervention_message
        else:
            print(f"üö´ AI WILL NOT RESPOND: AI decided not to intervene for room {room_id}")
        
        return should_intervene
        
    async def generate_response(self, room_id: str) -> Optional[str]:
        """Generate AI response using centralized decision"""
        if not self.client:
            print("‚ö†Ô∏è  Cannot generate AI response: OpenAI client not initialized")
            return None
            
        if room_id not in self.conversation_history:
            return None
            
        context = self.conversation_history[room_id]
        
        # Check if we have a pre-generated intervention message from centralized decision
        if hasattr(context, 'pending_intervention_message') and context.pending_intervention_message:
            intervention_message = context.pending_intervention_message
            
            print(f"‚úÖ USING CENTRALIZED INTERVENTION: {len(intervention_message.split())} words")
            print(f"   Preview: {intervention_message[:100]}{'...' if len(intervention_message) > 100 else ''}")
            
            # Update tracking
            context.last_ai_response = datetime.now()
            
            # Clear the pending message
            context.pending_intervention_message = None
            
            return intervention_message
        else:
            print("‚ö†Ô∏è  No centralized intervention message found")
            return None
    
    # Legacy research-based response generation removed - replaced by centralized LLM decision
            
    async def generate_speech(self, text: str) -> Optional[bytes]:
        """Generate speech audio from text using OpenAI TTS"""
        if not self.client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            response = self.client.audio.speech.create(
                model=self.voice_config["model"],
                voice=self.voice_config["voice"],
                input=text,
                speed=self.voice_config["speed"]
            )
            
            # Return the audio bytes
            return response.content
            
        except Exception as e:
            print(f"Error generating speech: {e}")
            return None

    async def generate_streaming_speech(self, text: str, room_id: str, message_id: str):
        """Generate streaming speech audio using OpenAI's streaming TTS API (following official docs)"""
        if not self.async_client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            # Signal start of streaming
            self.socketio.emit('ai_audio_stream_start', {
                'messageId': message_id,
                'room': room_id
            }, room=room_id, namespace='/ws')
            
            chunk_number = 0
            total_bytes_sent = 0
            
            print(f"üé§ Starting to stream audio for text: '{text[:50]}...'")
            
            # Use OpenAI's official streaming approach with AsyncOpenAI
            async with self.async_client.audio.speech.with_streaming_response.create(
                model=self.voice_config["model"],  # tts-1
                voice=self.voice_config["voice"],  # nova
                input=text,
                speed=self.voice_config["speed"],
                response_format="pcm"  # PCM for true real-time streaming
            ) as response:
                chunks_sent = []
                # Stream chunks directly as they arrive - TRUE real-time streaming
                async for chunk in response.iter_bytes(chunk_size=1024 * 2):  # 2KB chunks for PCM real-time
                    if chunk:
                        chunk_number += 1
                        total_bytes_sent += len(chunk)
                        chunk_base64 = base64.b64encode(chunk).decode('utf-8')
                        chunks_sent.append(chunk_number)
                        
                        print(f"üì¶ Streaming PCM chunk {chunk_number}: {len(chunk)} bytes (real-time)")
                        
                        # Send chunk immediately as it arrives from OpenAI
                        self.socketio.emit('ai_audio_chunk', {
                            'messageId': message_id,
                            'audioData': chunk_base64,
                            'chunkNumber': chunk_number,
                            'totalBytes': total_bytes_sent,
                            'room': room_id,
                            'isComplete': False,  # We don't know if this is the last chunk yet
                            'isRealtime': True,
                            'format': 'pcm'  # PCM format for true real-time streaming
                        }, room=room_id, namespace='/ws')
                        
                        # No artificial delay - stream as fast as data arrives from OpenAI

                
                # print(f"üèÅ Sending final chunk marker for chunk {final_chunk_number}")
                
                # Send a special "final chunk" marker
                if chunks_sent:
                    final_chunk_number = chunks_sent[-1]
                    print(f"üèÅ Sending final chunk marker for chunk {final_chunk_number}")
                    
                    # Send a special "final chunk" marker
                    self.socketio.emit('ai_audio_chunk', {
                        'messageId': message_id,
                        'audioData': '',  # Empty data
                        'chunkNumber': final_chunk_number,
                        'totalBytes': total_bytes_sent,
                        'room': room_id,
                        'isComplete': True,  # Mark as final
                        'isRealtime': True,
                        'format': 'pcm',
                        'isFinalMarker': True  # Special flag to indicate this is just a marker
                    }, room=room_id, namespace='/ws')

                # Signal completion - only that streaming is done, not that playback is done
                self.socketio.emit('ai_audio_complete', {
                    'messageId': message_id,
                    'room': room_id,
                    'totalChunks': chunk_number,
                    'totalBytes': total_bytes_sent,
                    'format': 'pcm'
                }, room=room_id, namespace='/ws')
                
                # DON'T send ai_audio_done here - let frontend determine when playback is actually finished
                
                print(f"‚úÖ Streamed {chunk_number} PCM chunks ({total_bytes_sent} bytes) in real-time for message {message_id}")
            return True
            
        except Exception as e:
            print(f"Error generating streaming speech: {e}")
            # Signal error
            self.socketio.emit('ai_audio_error', {
                'messageId': message_id,
                'room': room_id,
                'error': str(e)
            }, room=room_id, namespace='/ws')
            
            # CRITICAL: Even on error, signal that audio streaming is done
            self.socketio.emit('ai_audio_done', {
                'messageId': message_id,
                'room': room_id,
                'status': 'error'
            }, room=room_id, namespace='/ws')
            
            return None

    async def send_ai_message_with_audio(self, room_id: str, content: str):
        """Send an AI message to the chat room with streaming audio - optimized for speed"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': True,  # Will have streaming audio
            'isStreaming': True  # Indicate this is a streaming response
        }
        
        # Update last response time for cooldown tracking
        if room_id in self.conversation_history:
            context = self.conversation_history[room_id]
            context.last_ai_response = datetime.now()
            print(f"üîí AI RESPONSE: Tracking response time for cooldown in room {room_id}")
        
        # Send message immediately (don't wait for audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
        
        # Generate streaming audio in parallel (non-blocking)
        def generate_and_stream_audio():
            try:
                # Run the async audio streaming in a new event loop
                async def audio_task():
                    await self.generate_streaming_speech(content, room_id, message['id'])
                
                # Create and run new event loop for audio generation
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(audio_task())
                loop.close()
            except Exception as e:
                print(f"Error in streaming audio: {e}")
                # Fallback to simple notification that audio failed
                self.socketio.emit('ai_audio_error', {
                    'messageId': message['id'],
                    'room': room_id,
                    'error': 'Audio generation failed'
                }, room=room_id, namespace='/ws')
        
        # Start audio streaming in a separate thread
        threading.Thread(target=generate_and_stream_audio, daemon=True).start()
    
    def send_ai_message_text_only(self, room_id: str, content: str):
        """Send an AI message to the chat room without audio"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': False
        }
        
        # Update last response time for non-greeting messages
        if room_id in self.conversation_history:
            self.conversation_history[room_id].last_ai_response = datetime.now()
            
        # Emit the message to the room (no audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
    
    def send_ai_message(self, room_id: str, content: str):
        """Send an AI message to the chat room (optimized sync version)"""
        try:
            # Try to use existing event loop if available
            try:
                loop = asyncio.get_running_loop()
                asyncio.create_task(self.send_ai_message_with_audio(room_id, content))
                return
            except RuntimeError:
                # No running loop - use thread pool to avoid blocking
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.send_ai_message_with_audio(room_id, content))
                    return
        except Exception as e:
            # Fallback to simple text-only message
            self.send_ai_message_text_only(room_id, content)
        
    def process_message_sync(self, message_data: Dict[str, Any]):
        """Process a new message and potentially respond"""
        try:
            # Check if this is a direct AI mention BEFORE adding to context
            is_direct_mention = self._is_direct_ai_mention(message_data.get('content', ''))
            
            # Add message to context (this will handle direct mentions and return early)
            self.add_message_to_context(message_data)
            
            # Only start timer for non-direct mentions
            if not is_direct_mention:
                room_id = message_data.get('room')
                if room_id and room_id in self.conversation_history:
                    # Start new 5-second idle timer
                    self._schedule_idle_intervention(room_id)
                
        except Exception as e:
            print(f"Error processing message in AI agent: {e}")
            # Continue processing other messages
            
    def handle_code_update(self, room_id: str, code: str, language: str = "python"):
        """Handle code updates from the editor"""
        self.update_code_context(room_id, code, language)
        
    def handle_problem_update(self, room_id: str, problem_title: str, problem_description: str):
        """Handle problem description updates"""
        self.update_problem_context(room_id, problem_title, problem_description)
        
    def release_generation_lock(self, room_id: str, message_id: str = None):
        """Release AI generation lock when audio playback is complete (simplified for current architecture)"""
        if room_id in self.conversation_history:
            # In the current simplified architecture, we don't maintain a generation lock
            # The 5-second idle timer and cooldown period handle response timing
            print(f"üîì Audio playback complete for room {room_id} (message: {message_id})")
            # Could add any cleanup logic here if needed in the future
        else:
            print(f"‚ö†Ô∏è AI RESPONSE LOCK: No context found for room {room_id}")
    
    def join_room(self, room_id: str):
        """AI agent joins a room"""
        # Only send greeting if OpenAI client is available
        if not self.client:
            print(f"‚ö†Ô∏è  CodeBot cannot join room {room_id}: OpenAI client not initialized")
            return
            
        # Send a greeting message when joining
        greeting_messages = [
            # "Hi! I'm CodeBot, your AI pair programming assistant. I'll help with technical questions, encourage good planning, and facilitate your collaboration. Let's code together!",
            # "Hello! I'm here to support your pair programming session. I can provide hints when you're stuck, help with code review, and ensure both of you stay engaged. Ready to start?",
            "Welcome! I'm CodeBob, designed to enhance your pair programming experience. I'll offer technical guidance and help maintain productive collaboration."
        ]
        
        import random
        greeting = random.choice(greeting_messages)
        
        # Send greeting after a short delay (non-blocking)
        def send_greeting():
            time.sleep(2)  # Wait 2 seconds before greeting
            self.send_ai_message_text_only(room_id, greeting)
            
        # Start greeting in a separate thread to avoid blocking
        threading.Thread(target=send_greeting, daemon=True).start()

    def set_voice_config(self, voice: str = None, model: str = None, speed: float = None):
        """Update voice configuration for TTS"""
        if voice and voice in ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]:
            self.voice_config["voice"] = voice
        if model and model in ["tts-1", "tts-1-hd"]:
            self.voice_config["model"] = model
        if speed and 0.25 <= speed <= 4.0:
            self.voice_config["speed"] = speed

    # Legacy background monitoring removed - replaced by 5-second idle timer and centralized decision
    
    def _centralized_ai_decision(self, context: ConversationContext) -> tuple[bool, str]:
        """Simple centralized AI decision: Should intervene and what to say"""
        if not self.client:
            print("üö´ AI WILL NOT INTERVENE: No LLM client available")
            return False, ""
        
        # Get recent conversation context (last 5 messages)
        recent_conversation = ""
        for msg in context.messages[-5:]:
            recent_conversation += f"{msg.username}: {msg.content}\n"
        
        # Check if the last message contains direct AI mention
        last_message = context.messages[-1] if context.messages else None
        is_direct_mention = last_message and self._is_direct_ai_mention(last_message.content)
        
        # Adjust prompt based on whether this is a direct mention
        if is_direct_mention:
            prompt = f"""You are CodeBot, an AI pair programming assistant focused on LEARNING. The user has directly mentioned you with @AI or similar keyword.

                            Problem Context:
                            - Problem: {context.problem_title or "General coding"}
                            - Language: {context.programming_language}

                            Recent Conversation:
                            {recent_conversation}

                            Code Context:
                            {context.code_context[:300] if context.code_context else "No code visible"}

                            NATURAL TEACHING APPROACH - Be helpful while encouraging learning:
                            - Mix different response types: hints, encouragement, specific guidance, questions
                            - Be conversational and supportive, not just question-asking
                            - Adapt to their level: sometimes give direct help when appropriate
                            - Balance learning with actually being helpful

                            Response variety examples:
                            - Direct help: "Try using array.map() instead of a for loop here"
                            - Hint: "Look at the error message - it's telling you about a missing semicolon"
                            - Encouragement: "You're close! The logic is right, just check your syntax"
                            - Question: "What do you think that error means?"
                            - Specific tip: "Console.log the variable to see what value it actually has"

                            Response format:
                            - If you should help: "YES|[natural, helpful response that may include hints, tips, or direct guidance - 15-40 words]"
                            - If inappropriate request: "NO"

                            IMPORTANT: Be naturally helpful while encouraging learning. Don't always ask questions!

                            Your response:"""
        else:
            prompt = f"""You are CodeBot, an AI pair programming assistant focused on LEARNING. Should you help in this conversation?

                        Problem Context:
                        - Problem: {context.problem_title or "General coding"}
                        - Language: {context.programming_language}

                        Recent Conversation:
                        {recent_conversation}

                        Code Context:
                        {context.code_context[:300] if context.code_context else "No code visible"}

                        NATURAL INTERVENTION - Help appropriately without being pushy:
                        - If they're stuck or confused: Offer helpful hints or specific tips
                        - If they're discussing actively: Let them work it out
                        - If they need encouragement: Give positive reinforcement
                        - If there's a clear issue: Point it out gently

                        VARIED RESPONSE TYPES:
                        - Helpful tips: "Try adding console.log to see what's happening"
                        - Encouragement: "Good progress! You're on the right track"
                        - Specific hints: "That error usually means a missing bracket"
                        - Questions (sparingly): "What do you think might be causing that?"
                        - Direct guidance when stuck: "The issue is likely in line 5"

                        Response format:
                        - If you should help: "YES|[natural, varied response - hints, tips, or encouragement in 10-30 words]"
                        - If they're doing fine: "NO"

                        IMPORTANT: Mix response types. Don't always ask questions - sometimes just give helpful tips!

                        Your response:"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are CodeBot, a helpful pair programming assistant. Only intervene when truly helpful."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=150 if is_direct_mention else 100,  # Allow longer responses for direct mentions
                temperature=0.7
            )
            
            llm_response = response.choices[0].message.content.strip()
            
            if llm_response.startswith("YES|"):
                # Parse the response: YES|MESSAGE
                parts = llm_response.split("|", 1)
                if len(parts) >= 2:
                    intervention_message = parts[1]
                    mention_type = "DIRECT MENTION" if is_direct_mention else "IDLE INTERVENTION"
                    print(f"‚úÖ AI WILL INTERVENE ({mention_type}): {intervention_message[:50]}...")
                    return True, intervention_message
                else:
                    print(f"‚ùå LLM response format error: {llm_response}")
                    return False, ""
            else:
                mention_type = "direct mention" if is_direct_mention else "idle period"
                print(f"üö´ AI WILL NOT INTERVENE: Decided not to respond to {mention_type}")
                return False, ""
                
        except Exception as e:
            print(f"‚ùå Error in AI decision: {e}")
            return False, ""
    
    def _is_direct_ai_mention(self, message_content: str) -> bool:
        """Check if message contains direct AI mention keywords"""
        # Split into words for exact word matching (more efficient and accurate)
        words = message_content.lower().split()
        
        # Single-word AI mention keywords
        ai_keywords = {
            '@ai', '@codebot', 'codebot', 'bob', 'help'
        }
        
        # Check if any word is an AI keyword
        return any(word in ai_keywords for word in words)
    
    def _handle_direct_ai_mention(self, room_id: str):
        """Handle direct AI mention - respond immediately bypassing ALL restrictions"""
        context = self.conversation_history.get(room_id)
        if not context:
            return
            
        if not self.client:
            print("üö´ AI cannot respond to direct mention: No LLM client available")
            return
            
        print(f"üöÄ BYPASSING ALL RESTRICTIONS for direct AI mention in room {room_id}")
        
        # Force AI decision for direct mention (no restrictions)
        should_respond, message = self._centralized_ai_decision(context)
        
        if should_respond and message:
            # Update AI response timestamp (but no cooldown enforced for direct mentions)
            context.last_ai_response = datetime.now()
            
            # Send immediate AI response using proper chat message format
            self.send_ai_message(room_id, message)
            
            print(f"‚úÖ AI responded IMMEDIATELY to direct mention in room {room_id}: {message[:50]}...")
        else:
            # Even if LLM says no, we should respond to direct mentions with a helpful message
            fallback_message = "I'm here to help! What specific question do you have about your code or programming problem?"
            context.last_ai_response = datetime.now()
            self.send_ai_message(room_id, fallback_message)
            print(f"‚úÖ AI responded with fallback to direct mention in room {room_id}")
    
    # Legacy complex intervention methods removed - replaced by centralized LLM decision
    # Legacy research-based intervention methods removed - all replaced by centralized LLM decision

# Global AI agent instance
ai_agent = None

def init_ai_agent(socketio_instance):
    """Initialize the AI agent"""
    global ai_agent
    ai_agent = AIAgent(socketio_instance)
    return ai_agent

def get_ai_agent():
    """Get the global AI agent instance"""
    return ai_agent
