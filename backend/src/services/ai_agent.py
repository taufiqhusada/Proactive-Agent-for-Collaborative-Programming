"""
AI Agent Service - A proactive AI teammate for pair programming
Uses GPT-4o-mini to analyze conversations and provide helpful insights
"""

import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from openai import OpenAI, AsyncOpenAI
from dataclasses import dataclass
import threading
import time
import base64
import random
import concurrent.futures
import logging

@dataclass
class Message:
    id: str
    content: str
    username: str
    userId: str
    timestamp: str
    room: str
    isAutoGenerated: bool = False

@dataclass
class ConversationContext:
    messages: List[Message]
    room_id: str
    last_ai_response: Optional[datetime] = None
    code_context: str = ""
    programming_language: str = "python"
    problem_description: str = ""
    problem_title: str = ""
    
    # Simple timestamp tracking
    last_message_time: Optional[datetime] = None  # When the last message was received

class AIAgent:
    def __init__(self, socketio_instance):
        # Check if OpenAI API key is available
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("‚ö†Ô∏è  Warning: No OpenAI API key found. AI agent will be disabled.")
            print("   Set OPENAI_API_KEY in your .env file to enable AI responses.")
            self.client = None
        else:
            try:
                self.client = OpenAI(api_key=api_key)
                self.async_client = AsyncOpenAI(api_key=api_key)  # For streaming TTS
                print("‚úÖ AI Agent (CodeBot) initialized successfully!")
            except Exception as e:
                print(f"‚ùå Error initializing OpenAI client: {e}")
                print("   AI agent will be disabled.")
                self.client = None
                self.async_client = None
        
        self.socketio = socketio_instance
        self.conversation_history = {}  # room_id -> ConversationContext
        
        # Simple timing parameters
        self.response_cooldown = 15  # Minimum seconds between AI responses
        self.min_messages_before_response = 3  # Wait for at least 3 messages before responding
        self.max_context_messages = 10  # Keep last 10 messages for context
        
        # AI Agent identity and voice configuration
        self.agent_name = "CodeBot"
        self.agent_id = "ai_agent_codebot"
        self.voice_config = {
            "model": "tts-1",            # Use OpenAI's fast TTS model (tts-1 or tts-1-hd)
            "voice": "nova",             # Available: alloy, echo, fable, onyx, nova, shimmer
            "speed": 1.0                 # 0.25 to 4.0
        }
        
        # Simple timer tracking (much more efficient)
        self.pending_timers = {}  # room_id -> threading.Timer

        # Add execution validation tracking
        self.execution_attempts = {}  # Track attempts per room for graduated help
        self.validation_tasks = {}    # Track running validation tasks

    def _cancel_pending_intervention(self, room_id: str, reason: str):
        """Cancel any pending timer for a room"""
        if room_id in self.pending_timers:
            timer = self.pending_timers[room_id]
            timer.cancel()
            del self.pending_timers[room_id]
            print(f"üö´ CANCELLED timer ({reason}) in room {room_id}")
    
    def _schedule_idle_intervention(self, room_id: str):
        """Schedule a 5-second idle intervention timer using threading.Timer"""
        # Cancel existing timer
        self._cancel_pending_intervention(room_id, "new timer scheduled")
        
        def timer_callback():
            """Handle timer completion after 5 seconds"""
            try:
                print(f"‚è∞ 5-second timer completed for room {room_id}")
                
                # Clean up timer reference
                self.pending_timers.pop(room_id, None)
                
                # Check if we should respond
                if self.should_respond(room_id):
                    print(f"ü§ñ AI will respond after 5-second idle period in room {room_id}")
                    response = self._generate_response_sync(room_id)
                    if response:
                        self.send_ai_message(room_id, response)
                else:
                    print(f"üö´ No intervention needed after 5-second idle period for room {room_id}")
                        
            except Exception as e:
                print(f"‚ùå Timer callback error for room {room_id}: {e}")
        
        # Create and start timer
        timer = threading.Timer(5.0, timer_callback)
        timer.daemon = True
        timer.start()
        
        # Store timer reference
        self.pending_timers[room_id] = timer
        print(f"‚è±Ô∏è Started 5-second timer for room {room_id}")
    
    def _generate_response_sync(self, room_id: str) -> Optional[str]:
        """Synchronous response generation for timer callbacks"""
        if not self.client or room_id not in self.conversation_history:
            return None
            
        context = self.conversation_history[room_id]
        should_intervene, message = self._centralized_ai_decision(context)
        
        if should_intervene:
            context.last_ai_response = datetime.now()
            print(f"‚úÖ Generated response: {message[:50]}...")
            return message
        return None

        
    def add_message_to_context(self, message_data: Dict[str, Any]):
        """Add a new message to the conversation context with direct AI mention detection"""
        room_id = message_data.get('room')
        if not room_id:
            return
            
        # Skip AI's own messages
        if message_data.get('userId') == self.agent_id:
            return
            
        message = Message(
            id=message_data.get('id', ''),
            content=message_data.get('content', ''),
            username=message_data.get('username', 'Unknown'),
            userId=message_data.get('userId', ''),
            timestamp=message_data.get('timestamp', ''),
            room=room_id,
            isAutoGenerated=message_data.get('isAutoGenerated', False)
        )
        
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.messages.append(message)
        
        # Update last message time for 5-second idle timer
        context.last_message_time = datetime.now()
        
        # Cancel any pending intervention since user is active
        self._cancel_pending_intervention(room_id, "new message received")
        
        # Check for direct AI mention (@AI keyword) - PRIORITY RESPONSE
        if self._is_direct_ai_mention(message.content):
            print(f"üéØ DIRECT AI MENTION detected in room {room_id}: {message.content[:50]}...")
            # Respond immediately without waiting for 5-second timer
            self._handle_direct_ai_mention(room_id)
            # DO NOT start timer for direct mentions - return early
            if len(context.messages) > self.max_context_messages:
                context.messages = context.messages[-self.max_context_messages:]
            return
        
        # Keep only recent messages
        if len(context.messages) > self.max_context_messages:
            context.messages = context.messages[-self.max_context_messages:]
            
    def update_code_context(self, room_id: str, code: str, language: str = "python"):
        """Update the current code context for a room"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.code_context = code
        context.programming_language = language
        
        # Cancel any pending intervention since user is actively coding
        self._cancel_pending_intervention(room_id, "code update received")
        print(f"üñ•Ô∏è Code updated in room {room_id} - cancelled pending timer")
        
    def update_problem_context(self, room_id: str, problem_title: str, problem_description: str):
        """Update the current problem description for a room"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.problem_title = problem_title
        context.problem_description = problem_description

    def should_respond(self, room_id: str) -> bool:
        """Simple decision making for AI intervention after 5-second idle"""
        if room_id not in self.conversation_history:
            print(f"üö´ AI WILL NOT RESPOND: No conversation history for room {room_id}")
            return False
            
        context = self.conversation_history[room_id]

        # Check cooldown period
        if context.last_ai_response:
            time_since_last = datetime.now() - context.last_ai_response
            if time_since_last < timedelta(seconds=self.response_cooldown):
                print(f"üö´ AI WILL NOT RESPOND: Cooldown period ({time_since_last.total_seconds():.1f}s < {self.response_cooldown}s)")
                return False
                
        # Need minimum number of messages
        if len(context.messages) < self.min_messages_before_response:
            print(f"üö´ AI WILL NOT RESPOND: Not enough messages ({len(context.messages)} < {self.min_messages_before_response})")
            return False
        
        # Simple AI decision using centralized LLM
        should_intervene, intervention_message = self._centralized_ai_decision(context)
        
        if should_intervene:
            print(f"‚úÖ AI WILL RESPOND: Intervention decision made for room {room_id}")
            # Store the intervention message for generate_response to use
            context.pending_intervention_message = intervention_message
        else:
            print(f"üö´ AI WILL NOT RESPOND: AI decided not to intervene for room {room_id}")
        
        return should_intervene
        
    async def generate_response(self, room_id: str) -> Optional[str]:
        """Generate AI response using centralized decision"""
        if not self.client:
            print("‚ö†Ô∏è  Cannot generate AI response: OpenAI client not initialized")
            return None
            
        if room_id not in self.conversation_history:
            return None
            
        context = self.conversation_history[room_id]
        
        # Check if we have a pre-generated intervention message from centralized decision
        if hasattr(context, 'pending_intervention_message') and context.pending_intervention_message:
            intervention_message = context.pending_intervention_message
            
            print(f"‚úÖ USING CENTRALIZED INTERVENTION: {len(intervention_message.split())} words")
            print(f"   Preview: {intervention_message[:100]}{'...' if len(intervention_message) > 100 else ''}")
            
            # Update tracking
            context.last_ai_response = datetime.now()
            
            # Clear the pending message
            context.pending_intervention_message = None
            
            return intervention_message
        else:
            print("‚ö†Ô∏è  No centralized intervention message found")
            return None
    
    # Legacy research-based response generation removed - replaced by centralized LLM decision
            
    async def generate_speech(self, text: str) -> Optional[bytes]:
        """Generate speech audio from text using OpenAI TTS"""
        if not self.client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            response = self.client.audio.speech.create(
                model=self.voice_config["model"],
                voice=self.voice_config["voice"],
                input=text,
                speed=self.voice_config["speed"]
            )
            
            # Return the audio bytes
            return response.content
            
        except Exception as e:
            print(f"Error generating speech: {e}")
            return None

    async def generate_streaming_speech(self, text: str, room_id: str, message_id: str):
        """Generate streaming speech audio using OpenAI's streaming TTS API (following official docs)"""
        if not self.async_client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            # Signal start of streaming
            self.socketio.emit('ai_audio_stream_start', {
                'messageId': message_id,
                'room': room_id
            }, room=room_id, namespace='/ws')
            
            chunk_number = 0
            total_bytes_sent = 0
            
            print(f"üé§ Starting to stream audio for text: '{text[:50]}...'")
            
            # Use OpenAI's official streaming approach with AsyncOpenAI
            async with self.async_client.audio.speech.with_streaming_response.create(
                model=self.voice_config["model"],  # tts-1
                voice=self.voice_config["voice"],  # nova
                input=text,
                speed=self.voice_config["speed"],
                response_format="pcm"  # PCM for true real-time streaming
            ) as response:
                chunks_sent = []
                # Stream chunks directly as they arrive - TRUE real-time streaming
                async for chunk in response.iter_bytes(chunk_size=1024 * 2):  # 2KB chunks for PCM real-time
                    if chunk:
                        chunk_number += 1
                        total_bytes_sent += len(chunk)
                        chunk_base64 = base64.b64encode(chunk).decode('utf-8')
                        chunks_sent.append(chunk_number)
                        
                        print(f"üì¶ Streaming PCM chunk {chunk_number}: {len(chunk)} bytes (real-time)")
                        
                        # Send chunk immediately as it arrives from OpenAI
                        self.socketio.emit('ai_audio_chunk', {
                            'messageId': message_id,
                            'audioData': chunk_base64,
                            'chunkNumber': chunk_number,
                            'totalBytes': total_bytes_sent,
                            'room': room_id,
                            'isComplete': False,  # We don't know if this is the last chunk yet
                            'isRealtime': True,
                            'format': 'pcm'  # PCM format for true real-time streaming
                        }, room=room_id, namespace='/ws')
                        
                        # No artificial delay - stream as fast as data arrives from OpenAI

                
                # print(f"üèÅ Sending final chunk marker for chunk {final_chunk_number}")
                
                # Send a special "final chunk" marker
                if chunks_sent:
                    final_chunk_number = chunks_sent[-1]
                    print(f"üèÅ Sending final chunk marker for chunk {final_chunk_number}")
                    
                    # Send a special "final chunk" marker
                    self.socketio.emit('ai_audio_chunk', {
                        'messageId': message_id,
                        'audioData': '',  # Empty data
                        'chunkNumber': final_chunk_number,
                        'totalBytes': total_bytes_sent,
                        'room': room_id,
                        'isComplete': True,  # Mark as final
                        'isRealtime': True,
                        'format': 'pcm',
                        'isFinalMarker': True  # Special flag to indicate this is just a marker
                    }, room=room_id, namespace='/ws')

                # Signal completion - only that streaming is done, not that playback is done
                self.socketio.emit('ai_audio_complete', {
                    'messageId': message_id,
                    'room': room_id,
                    'totalChunks': chunk_number,
                    'totalBytes': total_bytes_sent,
                    'format': 'pcm'
                }, room=room_id, namespace='/ws')
                
                # DON'T send ai_audio_done here - let frontend determine when playback is actually finished
                
                print(f"‚úÖ Streamed {chunk_number} PCM chunks ({total_bytes_sent} bytes) in real-time for message {message_id}")
            return True
            
        except Exception as e:
            print(f"Error generating streaming speech: {e}")
            # Signal error
            self.socketio.emit('ai_audio_error', {
                'messageId': message_id,
                'room': room_id,
                'error': str(e)
            }, room=room_id, namespace='/ws')
            
            # CRITICAL: Even on error, signal that audio streaming is done
            self.socketio.emit('ai_audio_done', {
                'messageId': message_id,
                'room': room_id,
                'status': 'error'
            }, room=room_id, namespace='/ws')
            
            return None

    async def send_ai_message_with_audio(self, room_id: str, content: str):
        """Send an AI message to the chat room with streaming audio - optimized for speed"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': True,  # Will have streaming audio
            'isStreaming': True  # Indicate this is a streaming response
        }
        
        # Update last response time for cooldown tracking
        if room_id in self.conversation_history:
            context = self.conversation_history[room_id]
            context.last_ai_response = datetime.now()
            print(f"üîí AI RESPONSE: Tracking response time for cooldown in room {room_id}")
        
        # Send message immediately (don't wait for audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
        
        # Generate streaming audio in parallel (non-blocking)
        def generate_and_stream_audio():
            try:
                # Run the async audio streaming in a new event loop
                async def audio_task():
                    await self.generate_streaming_speech(content, room_id, message['id'])
                
                # Create and run new event loop for audio generation
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(audio_task())
                loop.close()
            except Exception as e:
                print(f"Error in streaming audio: {e}")
                # Fallback to simple notification that audio failed
                self.socketio.emit('ai_audio_error', {
                    'messageId': message['id'],
                    'room': room_id,
                    'error': 'Audio generation failed'
                }, room=room_id, namespace='/ws')
        
        # Start audio streaming in a separate thread
        threading.Thread(target=generate_and_stream_audio, daemon=True).start()
    
    def send_ai_message_text_only(self, room_id: str, content: str):
        """Send an AI message to the chat room without audio"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': False
        }
        
        # Update last response time for non-greeting messages
        if room_id in self.conversation_history:
            self.conversation_history[room_id].last_ai_response = datetime.now()
            
        # Emit the message to the room (no audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
    
    def send_ai_message(self, room_id: str, content: str):
        """Send an AI message to the chat room (optimized sync version)"""
        try:
            # Try to use existing event loop if available
            try:
                loop = asyncio.get_running_loop()
                asyncio.create_task(self.send_ai_message_with_audio(room_id, content))
                return
            except RuntimeError:
                # No running loop - use thread pool to avoid blocking
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.send_ai_message_with_audio(room_id, content))
                    return
        except Exception as e:
            # Fallback to simple text-only message
            self.send_ai_message_text_only(room_id, content)
        
    def process_message_sync(self, message_data: Dict[str, Any]):
        """Process a new message and potentially respond"""
        try:
            # Check if this is a direct AI mention BEFORE adding to context
            is_direct_mention = self._is_direct_ai_mention(message_data.get('content', ''))
            
            # Add message to context (this will handle direct mentions and return early)
            self.add_message_to_context(message_data)
            
            # Only start timer for non-direct mentions
            if not is_direct_mention:
                room_id = message_data.get('room')
                if room_id and room_id in self.conversation_history:
                    # Start new 5-second idle timer
                    self._schedule_idle_intervention(room_id)
                
        except Exception as e:
            print(f"Error processing message in AI agent: {e}")
            # Continue processing other messages
            
    def handle_code_update(self, room_id: str, code: str, language: str = "python"):
        """Handle code updates from the editor"""
        self.update_code_context(room_id, code, language)
        
    def handle_problem_update(self, room_id: str, problem_title: str, problem_description: str):
        """Handle problem description updates"""
        self.update_problem_context(room_id, problem_title, problem_description)
        
    def release_generation_lock(self, room_id: str, message_id: str = None):
        """Release AI generation lock when audio playback is complete (simplified for current architecture)"""
        if room_id in self.conversation_history:
            # In the current simplified architecture, we don't maintain a generation lock
            # The 5-second idle timer and cooldown period handle response timing
            print(f"üîì Audio playback complete for room {room_id} (message: {message_id})")
            # Could add any cleanup logic here if needed in the future
        else:
            print(f"‚ö†Ô∏è AI RESPONSE LOCK: No context found for room {room_id}")
    
    def join_room(self, room_id: str):
        """AI agent joins a room"""
        # Only send greeting if OpenAI client is available
        if not self.client:
            print(f"‚ö†Ô∏è  CodeBot cannot join room {room_id}: OpenAI client not initialized")
            return
            
        # Send a greeting message when joining
        greeting_messages = [
            # "Hi! I'm CodeBot, your AI pair programming assistant. I'll help with technical questions, encourage good planning, and facilitate your collaboration. Let's code together!",
            # "Hello! I'm here to support your pair programming session. I can provide hints when you're stuck, help with code review, and ensure both of you stay engaged. Ready to start?",
            "Welcome! I'm CodeBob, designed to enhance your pair programming experience. I'll offer technical guidance and help maintain productive collaboration."
        ]
        
        import random
        greeting = random.choice(greeting_messages)
        
        # Send greeting after a short delay (non-blocking)
        def send_greeting():
            time.sleep(2)  # Wait 2 seconds before greeting
            self.send_ai_message_text_only(room_id, greeting)
            
        # Start greeting in a separate thread to avoid blocking
        threading.Thread(target=send_greeting, daemon=True).start()

    def set_voice_config(self, voice: str = None, model: str = None, speed: float = None):
        """Update voice configuration for TTS"""
        if voice and voice in ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]:
            self.voice_config["voice"] = voice
        if model and model in ["tts-1", "tts-1-hd"]:
            self.voice_config["model"] = model
        if speed and 0.25 <= speed <= 4.0:
            self.voice_config["speed"] = speed

    # Legacy background monitoring removed - replaced by 5-second idle timer and centralized decision
    
    def _centralized_ai_decision(self, context: ConversationContext) -> tuple[bool, str]:
        """Simple centralized AI decision: Should intervene and what to say"""
        if not self.client:
            print("üö´ AI WILL NOT INTERVENE: No LLM client available")
            return False, ""
        
        # Get recent conversation context (last 5 messages)
        recent_conversation = ""
        for msg in context.messages[-5:]:
            recent_conversation += f"{msg.username}: {msg.content}\n"
        
        # Check if the last message contains direct AI mention
        last_message = context.messages[-1] if context.messages else None
        is_direct_mention = last_message and self._is_direct_ai_mention(last_message.content)
        
        # Adjust prompt based on whether this is a direct mention
        if is_direct_mention:
            prompt = f"""You are CodeBot, an AI pair programming assistant focused on LEARNING. The user has directly mentioned you with @AI or similar keyword.

                            Problem Context:
                            - Problem: {context.problem_title or "General coding"}
                            - Language: {context.programming_language}

                            Recent Conversation:
                            {recent_conversation}

                            Code Context:
                            {context.code_context[:300] if context.code_context else "No code visible"}

                            NATURAL TEACHING APPROACH - Be helpful while encouraging learning:
                            - Mix different response types: hints, encouragement, specific guidance, questions
                            - Be conversational and supportive, not just question-asking
                            - Adapt to their level: sometimes give direct help when appropriate
                            - Balance learning with actually being helpful

                            Response variety examples:
                            - Direct help: "Try using array.map() instead of a for loop here"
                            - Hint: "Look at the error message - it's telling you about a missing semicolon"
                            - Encouragement: "You're close! The logic is right, just check your syntax"
                            - Question: "What do you think that error means?"
                            - Specific tip: "Console.log the variable to see what value it actually has"

                            Response format:
                            - If you should help: "YES|[natural, helpful response that may include hints, tips, or direct guidance - 15-40 words]"
                            - If inappropriate request: "NO"

                            IMPORTANT: Be naturally helpful while encouraging learning. Don't always ask questions!

                            Your response:"""
        else:
            prompt = f"""You are CodeBot, an AI pair programming assistant focused on LEARNING. Should you help in this conversation?

                        Problem Context:
                        - Problem: {context.problem_title or "General coding"}
                        - Language: {context.programming_language}

                        Recent Conversation:
                        {recent_conversation}

                        Code Context:
                        {context.code_context[:300] if context.code_context else "No code visible"}

                        NATURAL INTERVENTION - Help appropriately without being pushy:
                        - If they're stuck or confused: Offer helpful hints or specific tips
                        - If they're discussing actively: Let them work it out
                        - If they need encouragement: Give positive reinforcement
                        - If there's a clear issue: Point it out gently

                        VARIED RESPONSE TYPES:
                        - Helpful tips: "Try adding console.log to see what's happening"
                        - Encouragement: "Good progress! You're on the right track"
                        - Specific hints: "That error usually means a missing bracket"
                        - Questions (sparingly): "What do you think might be causing that?"
                        - Direct guidance when stuck: "The issue is likely in line 5"

                        Response format:
                        - If you should help: "YES|[natural, varied response - hints, tips, or encouragement in 10-30 words]"
                        - If they're doing fine: "NO"

                        IMPORTANT: Mix response types. Don't always ask questions - sometimes just give helpful tips!

                        Your response:"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are CodeBot, a helpful pair programming assistant. Only intervene when truly helpful."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=150 if is_direct_mention else 100,  # Allow longer responses for direct mentions
                temperature=0.7
            )
            
            llm_response = response.choices[0].message.content.strip()
            
            if llm_response.startswith("YES|"):
                # Parse the response: YES|MESSAGE
                parts = llm_response.split("|", 1)
                if len(parts) >= 2:
                    intervention_message = parts[1]
                    mention_type = "DIRECT MENTION" if is_direct_mention else "IDLE INTERVENTION"
                    print(f"‚úÖ AI WILL INTERVENE ({mention_type}): {intervention_message[:50]}...")
                    return True, intervention_message
                else:
                    print(f"‚ùå LLM response format error: {llm_response}")
                    return False, ""
            else:
                mention_type = "direct mention" if is_direct_mention else "idle period"
                print(f"üö´ AI WILL NOT INTERVENE: Decided not to respond to {mention_type}")
                return False, ""
                
        except Exception as e:
            print(f"‚ùå Error in AI decision: {e}")
            return False, ""
    
    def _is_direct_ai_mention(self, message_content: str) -> bool:
        """Check if message contains direct AI mention keywords"""
        # Split into words for exact word matching (more efficient and accurate)
        words = message_content.lower().split()
        
        # Single-word AI mention keywords
        ai_keywords = {
            '@ai', '@codebot', 'codebot', 'bob', 'help'
        }
        
        # Check if any word is an AI keyword
        return any(word in ai_keywords for word in words)
    
    def _handle_direct_ai_mention(self, room_id: str):
        """Handle direct AI mention - respond immediately bypassing ALL restrictions"""
        context = self.conversation_history.get(room_id)
        if not context:
            return
            
        if not self.client:
            print("üö´ AI cannot respond to direct mention: No LLM client available")
            return
            
        print(f"üöÄ BYPASSING ALL RESTRICTIONS for direct AI mention in room {room_id}")
        
        # Force AI decision for direct mention (no restrictions)
        should_respond, message = self._centralized_ai_decision(context)
        
        if should_respond and message:
            # Update AI response timestamp (but no cooldown enforced for direct mentions)
            context.last_ai_response = datetime.now()
            
            # Send immediate AI response using proper chat message format
            self.send_ai_message(room_id, message)
            
            print(f"‚úÖ AI responded IMMEDIATELY to direct mention in room {room_id}: {message[:50]}...")
        else:
            # Even if LLM says no, we should respond to direct mentions with a helpful message
            fallback_message = "I'm here to help! What specific question do you have about your code or programming problem?"
            context.last_ai_response = datetime.now()
            self.send_ai_message(room_id, fallback_message)
            print(f"‚úÖ AI responded with fallback to direct mention in room {room_id}")
    
    def analyze_code_block(self, code: str, language: str, context: Dict[str, Any], problem_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Analyze a code block for potential issues and provide suggestions"""
        print(f"üîç Starting code analysis with OpenAI")
        
        # Always use OpenAI - assume it's available
        try:
            print("üöÄ Using OpenAI analysis")
            # Create analysis prompt with problem context
            analysis_prompt = self._create_code_analysis_prompt(code, language, context, problem_context)
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are an expert code reviewer and pair programming partner. Analyze code blocks for potential issues, bugs, security vulnerabilities, performance problems, and best practice violations. Consider the problem context when evaluating whether code is appropriate - partial solutions and work-in-progress code should be judged differently than complete solutions. Provide practical, actionable suggestions."},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            # Parse the response
            analysis_text = response.choices[0].message.content

            print(f"üîç Full OpenAI response: {analysis_text}")
            analysis = self._parse_code_analysis(analysis_text, code, context, problem_context)
            
            print(f"üìä Parsed analysis: {analysis}")
            
            return {
                'issues': analysis.get('issues', []),
                'suggestions': analysis.get('suggestions', []),
                'timestamp': datetime.now().isoformat(),
                'confidence': analysis.get('confidence', 'medium')
            }
            
        except Exception as e:
            print(f"‚ùå Error in OpenAI code analysis: {e}")
            # Return empty result if OpenAI fails
            return {
                'issues': [],
                'suggestions': [],
                'timestamp': datetime.now().isoformat(),
                'confidence': 'low'
            }
    
    def _mock_code_analysis(self, code: str, language: str, context: Dict[str, Any], problem_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Provide mock analysis when OpenAI is not available"""
        issues = []
        
        print(f"üîç Mock analysis with problem context: {problem_context}")
        
        # Context-aware analysis based on problem
        if problem_context:
            problem_title = problem_context.get('title', '').lower()
            problem_description = problem_context.get('description', '').lower()
            
            # Check if code aligns with problem expectations
            if 'two sum' in problem_title or 'sum' in problem_title:
                if 'for' in code and 'range' in code:
                    # Check for undefined variables in range()
                    import re
                    range_matches = re.findall(r'range\(([^)]+)\)', code)
                    for match in range_matches:
                        if match.strip() not in ['len(arr)', 'len(nums)', 'len(array)'] and not match.strip().isdigit():
                            # Check if it's a variable that might be undefined
                            if match.strip() in ['n', 'm', 'size', 'length'] and match.strip() not in code.replace('range', ''):
                                issues.append({
                                    'id': 'undefined_variable',
                                    'type': 'Bug Risk',
                                    'severity': 'high',
                                    'title': f'Undefined Variable: {match.strip()}',
                                    'description': f'The variable "{match.strip()}" is used in range() but not defined. This will cause a NameError.',
                                    'suggestedFix': {
                                        'description': f'Replace {match.strip()} with len(arr) or define the variable',
                                        'code': f'for i in range(len(arr)):  # Use len(arr) instead of {match.strip()}',
                                        'explanation': 'Use len(arr) to get the actual length of the array'
                                    }
                                })
                    
                    # Check for Two Sum logic errors
                    if 'range(i' not in code and 'range(j' not in code:
                        # Check if both loops use the same range without offset
                        if code.count('range(') >= 2:
                            lines = code.split('\n')
                            for i, line in enumerate(lines):
                                if 'for j in range(' in line and 'range(i' not in line:
                                    issues.append({
                                        'id': 'two_sum_logic_error',
                                        'type': 'Logic',
                                        'severity': 'medium',
                                        'title': 'Two Sum Logic Error: Using Same Element Twice',
                                        'description': 'Your inner loop should start from i+1 to avoid using the same element twice and to prevent redundant checks.',
                                        'suggestedFix': {
                                            'description': 'Start the inner loop from i+1 instead of 0',
                                            'code': 'for j in range(i + 1, len(arr)):',
                                            'explanation': 'This ensures we never use the same element twice and avoid checking the same pair multiple times'
                                        }
                                    })
                    
                    # This is likely a brute force approach for Two Sum
                    if code.count('for') >= 2:  # Nested loops
                        issues.append({
                            'id': 'two_sum_inefficient',
                            'type': 'Performance',
                            'severity': 'medium',
                            'title': 'Two Sum: Consider Hash Map Approach',
                            'description': f'For the Two Sum problem, your nested loop approach has O(n¬≤) time complexity. A hash map approach would be O(n).',
                            'suggestedFix': {
                                'description': 'Use a hash map to store numbers and their indices for O(1) lookup',
                                'code': '''def two_sum(nums, target):
    seen = {}
    for i, num in enumerate(nums):
        complement = target - num
        if complement in seen:
            return [seen[complement], i]
        seen[num] = i
    return []''',
                                'explanation': 'Hash map provides O(1) average lookup time, reducing overall complexity from O(n¬≤) to O(n)'
                            }
                        })
                    elif 'return' not in code:
                        issues.append({
                            'id': 'missing_return',
                            'type': 'Logic',
                            'severity': 'low',
                            'title': 'Missing Return Statement',
                            'description': 'Your Two Sum solution appears to be missing a return statement.',
                            'suggestedFix': {
                                'description': 'Add a return statement to return the indices',
                                'code': 'return [i, j]  # Return indices when sum equals target',
                                'explanation': 'The problem expects you to return the indices of the two numbers that add up to the target'
                            }
                        })
        
        # General bug detection patterns
        # Check for common undefined variable patterns
        import re
        
        # Check for variables used in range() that might be undefined
        range_vars = re.findall(r'range\(([^)]+)\)', code)
        for var in range_vars:
            var = var.strip()
            if var in ['n', 'm', 'size', 'length'] and var not in code.replace('range', '').replace('(', '').replace(')', ''):
                issues.append({
                    'id': f'undefined_var_{var}',
                    'type': 'Bug Risk',
                    'severity': 'high',
                    'title': f'Undefined Variable: {var}',
                    'description': f'Variable "{var}" is used but not defined. This will cause a NameError when the code runs.',
                    'suggestedFix': {
                        'description': f'Define {var} or use len(array_name) instead',
                        'code': f'n = len(arr)  # Define {var} before using it',
                        'explanation': 'Make sure all variables are defined before use'
                    }
                })
        
        # General pattern-based analysis
        if 'password' in code.lower() and ('=' in code or ':' in code):
            issues.append({
                'id': 'hardcoded_password',
                'type': 'Security',
                'severity': 'high',
                'title': 'Hardcoded Password Detected',
                'description': 'Hardcoded passwords in source code pose security risks. Use environment variables or secure configuration instead.',
                'line': context.get('cursorLine', 1),
                'codeSnippet': code.split('\n')[0] if code else '',
                'suggestedFix': {
                    'description': 'Use environment variables for sensitive data',
                    'code': 'password = os.getenv("DB_PASSWORD")'
                }
            })
        
        if 'SELECT' in code and '+' in code and 'f"' in code:
            issues.append({
                'id': 'sql_injection',
                'type': 'Security',
                'severity': 'high',
                'title': 'Potential SQL Injection',
                'description': 'String concatenation in SQL queries can lead to SQL injection attacks. Use parameterized queries instead.',
                'line': context.get('cursorLine', 1),
                'codeSnippet': code.split('\n')[0] if code else '',
                'suggestedFix': {
                    'description': 'Use parameterized queries',
                    'code': 'cursor.execute("SELECT * FROM users WHERE id = %s", (user_id,))'
                }
            })
        
        if 'for' in code and 'for' in code[code.find('for')+3:]:
            issues.append({
                'id': 'nested_loops',
                'type': 'Performance',
                'severity': 'medium',
                'title': 'Nested Loops Detected',
                'description': 'Multiple nested loops can impact performance with large datasets. Consider optimization.',
                'line': context.get('cursorLine', 1),
                'codeSnippet': code.split('\n')[0] if code else '',
                'suggestedFix': {
                    'description': 'Consider using list comprehensions or vectorized operations',
                    'code': '# Use list comprehension or optimize algorithm'
                }
            })
        
        if '.' in code and 'return' in code and 'if' not in code:
            issues.append({
                'id': 'null_reference',
                'type': 'Bug Risk',
                'severity': 'medium',
                'title': 'Potential Null Reference',
                'description': 'Object method calls without null checks can cause runtime errors.',
                'line': context.get('cursorLine', 1),
                'codeSnippet': code.split('\n')[0] if code else '',
                'suggestedFix': {
                    'description': 'Add null check before method calls',
                    'code': 'if user is not None:\n    return user.name'
                }
            })
        
        return {
            'issues': issues,
            'suggestions': [],
            'timestamp': datetime.now().isoformat(),
            'confidence': 'medium' if issues else 'low'
        }
    
    def _create_code_analysis_prompt(self, code: str, language: str, context: Dict[str, Any], problem_context: Optional[Dict[str, Any]] = None) -> str:
        """Create a comprehensive prompt for code analysis"""
        
        problem_info = ""
        if problem_context:
            problem_info = f"""
                            PROBLEM CONTEXT:
                            Title: {problem_context.get('title', 'Unknown')}
                            Description: {problem_context.get('description', 'No description provided')}

                            IMPORTANT: This code is being written to solve the above problem. Consider whether the approach is:
                            - Appropriate for the problem requirements
                            - A reasonable work-in-progress solution
                            - Following expected algorithmic patterns for this type of problem
                            - Missing key components needed for the solution

                            """
        
        return f"""
                Analyze this {language} code block for learning purposes:

                ```{language}
                {code}
                ```

                Context:
                - Lines: {context.get('startLine', 1)}-{context.get('endLine', 1)}
                - Cursor position: Line {context.get('cursorLine', 1)}

                {problem_info}

                IMPORTANT: Provide ONE consolidated issue that covers all problems found.
                Be concise and educational - focus on learning.

                Analyze for:
                1. **Undefined Variables**: Variables used but not defined
                2. **Logic Errors**: Algorithm mistakes, wrong loop bounds  
                3. **Performance**: Inefficient approaches
                4. **Bug Risks**: Index errors, type issues

                Combine multiple problems into ONE educational message.

                Format as JSON:
                {{
                "issue": {{
                    "title": "Brief title covering main problem(s)",
                    "description": "Concise explanation of what's wrong (1-2 sentences)",
                    "hint": "Quick practical solution (1 sentence)"
                }}
                }}

                If code is perfect, return: {{"issue": null}}

                Examples:
                - "Undefined Variables: Using 'n' and 'm' without defining them. Hint: Use len(arr) instead."
                - "Inefficient Algorithm: O(n¬≤) nested loops. Hint: Try hash map for O(n) solution."
                """
    
    def _parse_code_analysis(self, analysis_text: str, code: str, context: Dict[str, Any], problem_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Parse the AI response and create structured analysis"""
        try:
            import json
            
            # Simple slice to remove ```json and ``` from the response
            cleaned_text = analysis_text.strip()
            if cleaned_text.startswith('```json') and cleaned_text.endswith('```'):
                cleaned_text = cleaned_text[7:-4].strip()
            
            analysis = json.loads(cleaned_text)
            print(analysis)
            
            # Handle single-issue format
            if 'issue' in analysis:
                issue_data = analysis.get('issue')
                if issue_data is None:
                    # No issues found
                    return {'issues': []}
                
                # Convert single issue to array format for frontend
                single_issue = {
                    'id': f"issue_{hash(issue_data.get('title', 'unknown'))}",
                    'type': 'Code Review',
                    'severity': 'medium',
                    'title': issue_data.get('title', 'Code Issue'),
                    'description': issue_data.get('description', 'No description provided'),
                    'line': context.get('cursorLine', 1),
                    'codeSnippet': code.split('\n')[0] if code else '',
                    'suggestedFix': {
                        'description': issue_data.get('hint', 'No hint provided'),
                        'code': '',
                        'explanation': issue_data.get('hint', '')
                    }
                }
                
                return {'issues': [single_issue]}
            
            # Fallback for any other format
            return {'issues': []}
            
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON parsing error: {e}")
            print(f"üîç Raw response: {analysis_text}")
            return {'issues': []}
        except Exception as e:
            print(f"‚ùå Unexpected error: {e}")
            return {'issues': []}
    
    async def analyze_code_execution_async(self, code: str, execution_result: Dict[str, Any], 
                                         problem_context: Optional[str] = None, 
                                         room_id: Optional[str] = None) -> Dict[str, Any]:
        """Asynchronously analyze code execution results against problem requirements"""
        if not self.async_client:
            return {"needs_help": False, "message": "AI analysis unavailable"}
            
        try:
            # Build analysis prompt
            analysis_prompt = f"""You are an AI programming tutor analyzing student code execution.

                                    Code executed:
                                    ```
                                    {code}
                                    ```

                                    Execution Result:
                                    - Success: {execution_result.get('success', False)}
                                    - Output: {execution_result.get('output', '')}
                                    - Error: {execution_result.get('error', '')}

                                    Problem Context: {problem_context or 'No specific problem provided'}

                                    Analyze if this code correctly solves the problem. Respond with JSON only:
                                    {{
                                        "is_correct": boolean,
                                        "needs_help": boolean,
                                        "confidence": number (0-100),
                                        "issues": ["specific issues if any"],
                                        "help_type": "hint|debug|guidance|encouragement",
                                        "message": "encouraging message or offer to help (max 100 chars)"
                                    }}

                                    Be encouraging. Only offer help if there are clear issues or incorrect results."""

            response = await self.async_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": analysis_prompt}],
                max_tokens=300,
                temperature=0.3
            )

            print(response)
            
            analysis_text = response.choices[0].message.content.strip()
            return self._parse_execution_analysis(analysis_text)
            
        except Exception as e:
            logging.error(f"Error in async code execution analysis: {e}")
            return {
                "is_correct": False,
                "needs_help": True,
                "confidence": 0,
                "issues": ["Analysis failed"],
                "help_type": "debug",
                "message": "Need help with your code? ü§ñ"
            }

    def _parse_execution_analysis(self, analysis_text: str) -> Dict[str, Any]:
        """Parse the execution analysis response"""
        try:
            # Try to extract JSON from the response
            json_start = analysis_text.find('{')
            json_end = analysis_text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = analysis_text[json_start:json_end]
                return json.loads(json_str)
            else:
                # Fallback response
                return {
                    "is_correct": False,
                    "needs_help": True,
                    "confidence": 50,
                    "issues": ["Parse failed"],
                    "help_type": "guidance",
                    "message": "Want help with your code? üîç"
                }
                
        except Exception as e:
            logging.error(f"Error parsing execution analysis: {e}")
            return {
                "is_correct": False,
                "needs_help": True,
                "confidence": 0,
                "issues": ["Parse error"],
                "help_type": "debug",
                "message": "Code issue? Let me help! üõ†Ô∏è"
            }

    def _parse_execution_analysis_fast(self, analysis_text: str) -> Dict[str, Any]:
        """Fast parser for optimized execution analysis"""
        try:
            # Try to extract JSON
            json_start = analysis_text.find('{')
            json_end = analysis_text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = analysis_text[json_start:json_end]
                return json.loads(json_str)
            else:
                # Quick fallback
                return {
                    "needs_help": True,
                    "is_correct": False,
                    "help_message": "Need help? ü§ñ"
                }
                
        except Exception:
            return {
                "needs_help": True,
                "is_correct": False,
                "help_message": "Having trouble? üêõ"
            }

    async def handle_code_execution_validation_optimized(self, room_id: str, code: str, 
                                                        execution_result: Dict[str, Any]) -> None:
        """OPTIMIZED: Single AI call, faster message delivery"""
        try:
            # Get current problem context
            context = self.conversation_history.get(room_id, ConversationContext([], room_id))
            problem_context = context.problem_description or context.problem_title
            
            # OPTIMIZATION: Single AI call that analyzes AND generates help message
            analysis = await self.analyze_code_execution_async_optimized(
                code, execution_result, problem_context, room_id
            )
            
            print(f"üîç Optimized analysis result: {analysis}")

            # Track attempts for graduated help
            if room_id not in self.execution_attempts:
                self.execution_attempts[room_id] = 0
                
            # Only offer help if needed and not correct
            if analysis.get('needs_help', False) and not analysis.get('is_correct', True):
                self.execution_attempts[room_id] += 1
                
                # OPTIMIZATION: Skip the wait and activity check for faster response
                help_message = analysis.get('help_message', '')
                if help_message.strip():
                    # OPTIMIZATION: Direct socket emit instead of multiple thread/loop creation
                    await self._send_help_message_direct(room_id, help_message)
            else:
                # Reset attempts if code is correct
                self.execution_attempts[room_id] = 0
                
        except Exception as e:
            logging.error(f"Error in optimized code execution validation: {e}")

    async def analyze_code_execution_async_optimized(self, code: str, execution_result: Dict[str, Any], 
                                                   problem_context: Optional[str] = None, 
                                                   room_id: Optional[str] = None) -> Dict[str, Any]:
        """OPTIMIZED: Single AI call for analysis + help message generation"""
        if not self.async_client:
            return {"needs_help": False, "help_message": ""}
            
        try:
            # Get attempt count for context
            attempts = self.execution_attempts.get(room_id, 1) if room_id else 1
            
            # OPTIMIZATION: Include problem context but keep prompt concise
            prompt = f"""Analyze code execution for correctness:

                        Code: {code}
                        Problem: {problem_context or 'General coding task'}
                        Success: {execution_result.get('success', False)}
                        Output: {execution_result.get('output', '')}
                        Error: {execution_result.get('error', '')}
                        Attempt: {attempts}

                        Check: Does code solve the problem correctly? Any errors?

                        JSON response:
                        {{"needs_help": boolean, "is_correct": boolean, "help_message": "brief help (max 25 words) or empty if correct"}}"""

            response = await self.async_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=50,  # Slightly higher for problem-aware analysis
                temperature=0.3  # Very low for consistency
            )
            
            return self._parse_execution_analysis_fast(response.choices[0].message.content.strip())
            
        except Exception as e:
            logging.error(f"Error in optimized analysis: {e}")
            return {"needs_help": True, "is_correct": False, "help_message": "Need help? üêõ"}

    async def _send_help_message_direct(self, room_id: str, message: str) -> None:
        """OPTIMIZED: Direct message sending for execution help with special styling"""
        try:
            # Use the execution help message method for different styling
            await self.send_ai_execution_help_message(room_id, message)
        except Exception as e:
            logging.error(f"Error in direct help send: {e}")

    def start_execution_validation_optimized(self, room_id: str, code: str, execution_result: Dict[str, Any]) -> None:
        """OPTIMIZED: Start validation with minimal overhead"""
        try:
            if self.socketio:
                # Use the optimized validation method
                self.socketio.start_background_task(
                    self._run_async_validation_optimized, room_id, code, execution_result
                )
                print(f"üöÄ Started optimized AI validation for room {room_id}")
        except Exception as e:
            logging.error(f"Error starting optimized validation: {e}")

    def _run_async_validation_optimized(self, room_id: str, code: str, execution_result: Dict[str, Any]) -> None:
        """OPTIMIZED: Single event loop creation"""
        try:
            # Create new event loop for this thread (only once)
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # Run the optimized validation
            loop.run_until_complete(
                self.handle_code_execution_validation_optimized(room_id, code, execution_result)
            )
            
        except Exception as e:
            logging.error(f"Error in optimized validation thread: {e}")
        finally:
            try:
                loop.close()
            except:
                pass

    async def send_ai_execution_help_message(self, room_id: str, content: str):
        """Send an AI execution help message with different styling"""
        message = {
            'id': f"ai_exec_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'isExecutionHelp': True,  # New field to identify execution help messages
            'hasAudio': True,  # Will have streaming audio
            'isStreaming': True  # Indicate this is a streaming response
        }
        
        # Update last response time for cooldown tracking
        if room_id in self.conversation_history:
            context = self.conversation_history[room_id]
            context.last_ai_response = datetime.now()
            print(f"üîí AI EXECUTION HELP: Tracking response time for cooldown in room {room_id}")
        
        # Send message immediately (don't wait for audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
        
        # Generate streaming audio in parallel (non-blocking)
        def generate_and_stream_audio():
            try:
                # Run the async audio streaming in a new event loop
                async def audio_task():
                    await self.generate_streaming_speech(content, room_id, message['id'])
                
                # Create and run new event loop for audio generation
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(audio_task())
                loop.close()
            except Exception as e:
                print(f"Error in streaming audio: {e}")
                # Fallback to simple notification that audio failed
                self.socketio.emit('ai_audio_error', {
                    'messageId': message['id'],
                    'room': room_id,
                    'error': 'Audio generation failed'
                }, room=room_id, namespace='/ws')
        
        # Start audio streaming in a separate thread
        threading.Thread(target=generate_and_stream_audio, daemon=True).start()

# Global AI agent instance
ai_agent = None

def init_ai_agent(socketio_instance):
    """Initialize the AI agent"""
    global ai_agent
    ai_agent = AIAgent(socketio_instance)
    return ai_agent

def get_ai_agent():
    """Get the global AI agent instance"""
    return ai_agent
