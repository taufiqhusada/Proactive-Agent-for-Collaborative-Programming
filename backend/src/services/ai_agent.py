"""
AI Agent Service - A proactive AI teammate for pair programming
Uses GPT-4o-mini to analyze conversations and provide helpful insights
"""

import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from openai import OpenAI, AsyncOpenAI
from dataclasses import dataclass
import threading
import time
import base64
import random
import concurrent.futures

@dataclass
class Message:
    id: str
    content: str
    username: str
    userId: str
    timestamp: str
    room: str
    isAutoGenerated: bool = False

@dataclass
class ConversationContext:
    messages: List[Message]
    room_id: str
    last_ai_response: Optional[datetime] = None
    code_context: str = ""
    programming_language: str = "python"
    problem_description: str = ""
    problem_title: str = ""
    
    # Research-based tracking
    last_activity_time: Optional[datetime] = None
    silence_start: Optional[datetime] = None
    consecutive_errors: int = 0
    last_error_time: Optional[datetime] = None
    # Topic tracking removed - centralized LLM handles all conversation analysis
    user_participation: Dict[str, int] = None  # user_id -> message_count
    dominant_user: Optional[str] = None
    has_planned: bool = False
    last_code_review_time: Optional[datetime] = None
    needs_reflection: bool = False
    
    # Smart intervention tracking to prevent annoyance
    interventions_sent: Dict[str, datetime] = None  # intervention_type -> last_sent_time
    pending_ai_response: bool = False  # True if AI sent a message and waiting for user response
    last_user_response_time: Optional[datetime] = None  # When users last responded to AI
    intervention_escalation_level: int = 0  # 0=first attempt, 1=second attempt, etc.
    
    # Concurrency control to prevent voice overlap
    ai_generating_response: bool = False  # True if AI is currently generating a response
    ai_response_lock_time: Optional[datetime] = None  # When AI started generating response
    
    def __post_init__(self):
        if self.user_participation is None:
            self.user_participation = {}
        if self.interventions_sent is None:
            self.interventions_sent = {}

class AIAgent:
    def __init__(self, socketio_instance):
        # Check if OpenAI API key is available
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("‚ö†Ô∏è  Warning: No OpenAI API key found. AI agent will be disabled.")
            print("   Set OPENAI_API_KEY in your .env file to enable AI responses.")
            self.client = None
        else:
            try:
                self.client = OpenAI(api_key=api_key)
                self.async_client = AsyncOpenAI(api_key=api_key)  # For streaming TTS
                print("‚úÖ AI Agent (CodeBot) initialized successfully!")
            except Exception as e:
                print(f"‚ùå Error initializing OpenAI client: {e}")
                print("   AI agent will be disabled.")
                self.client = None
                self.async_client = None
        
        self.socketio = socketio_instance
        self.conversation_history = {}  # room_id -> ConversationContext
        
        # Research-based timing parameters
        self.response_cooldown = 15  # Minimum seconds between AI responses
        self.min_messages_before_response = 3  # Wait for at least 3 messages before responding
        self.max_context_messages = 10  # Keep last 10 messages for context
        
        # Research-based intervention thresholds
        self.silence_threshold = 30  # 30 seconds silence threshold (from research)
        # Misdirection threshold removed - centralized LLM handles all conversation analysis
        self.error_threshold = 3  # 3 identical errors trigger intervention
        self.dominance_threshold = 0.7  # If one user > 70% of messages
        self.code_review_interval = 5  # Review code every 5 seconds when coding
        
        # AI Agent identity and voice configuration
        self.agent_name = "CodeBot"
        self.agent_id = "ai_agent_codebot"
        self.voice_config = {
            "model": "tts-1",            # Use OpenAI's fast TTS model (tts-1 or tts-1-hd)
            "voice": "nova",             # Available: alloy, echo, fable, onyx, nova, shimmer
            "speed": 1.0                 # 0.25 to 4.0
        }
        
        # Start background monitoring for research-based interventions
        self._start_research_based_monitoring()
        
        # Centralized intervention tracking
        self.last_ai_intervention_time = None
        

        
    def add_message_to_context(self, message_data: Dict[str, Any]):
        """Add a new message to the conversation context with research-based tracking"""
        room_id = message_data.get('room')
        if not room_id:
            return
            
        # Skip AI's own messages
        if message_data.get('userId') == self.agent_id:
            return
            
        message = Message(
            id=message_data.get('id', ''),
            content=message_data.get('content', ''),
            username=message_data.get('username', 'Unknown'),
            userId=message_data.get('userId', ''),
            timestamp=message_data.get('timestamp', ''),
            room=room_id,
            isAutoGenerated=message_data.get('isAutoGenerated', False)
        )
        
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.messages.append(message)
        
        # Research-based tracking updates
        current_time = datetime.now()
        context.last_activity_time = current_time
        context.silence_start = None  # Reset silence tracking
        
        # Smart intervention management: Reset pending state when users respond
        if context.pending_ai_response:
            context.pending_ai_response = False
            context.last_user_response_time = current_time
            context.intervention_escalation_level = 0  # Reset escalation after user response
            print(f"‚úÖ USER RESPONDED: Reset AI intervention state for room {room_id}")
            print(f"   User: {message.username}")
            print(f"   Message: {message.content[:50]}{'...' if len(message.content) > 50 else ''}")
            print(f"   Escalation reset to: {context.intervention_escalation_level}")
        
        # Track user participation for imbalance detection
        user_id = message.userId
        if user_id not in context.user_participation:
            context.user_participation[user_id] = 0
        context.user_participation[user_id] += 1
        
        # Check for dominance (research: skill level mismatch issues)
        self._check_user_dominance(context)
        
        # Track error patterns
        self._track_error_patterns(context, message)
        
        # Keep only the most recent messages
        if len(context.messages) > self.max_context_messages:
            context.messages = context.messages[-self.max_context_messages:]
            
    def update_code_context(self, room_id: str, code: str, language: str = "python"):
        """Update the current code context for a room with research-based tracking"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.code_context = code
        context.programming_language = language
        context.last_code_change = datetime.now()
        
        # Mark that they might need reflection if substantial code is written
        if len(code) > 100:  # Substantial code
            context.needs_reflection = True
        
    def update_problem_context(self, room_id: str, problem_title: str, problem_description: str):
        """Update the current problem description for a room"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.problem_title = problem_title
        context.problem_description = problem_description

    def should_respond(self, room_id: str) -> bool:
        """Research-based decision making for AI intervention with concurrency control"""
        if room_id not in self.conversation_history:
            print(f"üö´ AI WILL NOT RESPOND: No conversation history for room {room_id}")
            return False
            
        context = self.conversation_history[room_id]

        # CRITICAL: Don't respond if AI is already generating a response
        if context.ai_generating_response:
            print(f"üö´ AI WILL NOT RESPOND: Already generating response for room {room_id}")
            return False
        
        # Check if AI response lock has expired (safety mechanism)
        if context.ai_response_lock_time:
            lock_duration = (datetime.now() - context.ai_response_lock_time).total_seconds()
            if lock_duration > 30:  # 30 second safety timeout
                print(f"‚ö†Ô∏è  AI response lock expired after {lock_duration:.1f}s, resetting")
                context.ai_generating_response = False
                context.ai_response_lock_time = None

        # Check cooldown period
        if context.last_ai_response:
            time_since_last = datetime.now() - context.last_ai_response
            if time_since_last < timedelta(seconds=self.response_cooldown):
                print(f"üö´ AI WILL NOT RESPOND: Cooldown period ({time_since_last.total_seconds():.1f}s < {self.response_cooldown}s)")
                return False
                
        # Need minimum number of messages
        if len(context.messages) < self.min_messages_before_response:
            print(f"üö´ AI WILL NOT RESPOND: Not enough messages ({len(context.messages)} < {self.min_messages_before_response})")
            return False
        
        # Research-based intervention logic
        should_intervene, intervention_message = self._should_intervene_based_on_research(context)
        
        if should_intervene:
            print(f"‚úÖ AI WILL RESPOND: Intervention decision made for room {room_id}")
            print(f"   Recent messages: {len(context.messages[-3:])}")
            print(f"   Last message: {context.messages[-1].content[:50]}{'...' if len(context.messages[-1].content) > 50 else ''}")
            print(f"   Intervention Message Preview: {intervention_message[:50]}{'...' if len(intervention_message) > 50 else ''}")
            
            # Store the intervention message for generate_response to use
            context.pending_intervention_message = intervention_message
            
            # Lock AI response generation to prevent concurrent responses
            context.ai_generating_response = True
            context.ai_response_lock_time = datetime.now()
            print(f"üîí AI RESPONSE LOCK: Acquired for room {room_id}")
        else:
            print(f"üö´ AI WILL NOT RESPOND: Intervention not needed for room {room_id}")
        
        return should_intervene
        
    async def generate_response(self, room_id: str) -> Optional[str]:
        """Generate research-based AI response using centralized intervention decision"""
        if not self.client:
            print("‚ö†Ô∏è  Cannot generate AI response: OpenAI client not initialized")
            return None
            
        if room_id not in self.conversation_history:
            return None
            
        context = self.conversation_history[room_id]
        
        # Check if we have a pre-generated intervention message from centralized decision
        if hasattr(context, 'pending_intervention_message') and context.pending_intervention_message:
            intervention_message = context.pending_intervention_message
            
            print(f"‚úÖ USING CENTRALIZED INTERVENTION: {len(intervention_message.split())} words")
            print(f"   Preview: {intervention_message[:100]}{'...' if len(intervention_message) > 100 else ''}")
            
            # Update tracking
            self.last_ai_intervention_time = datetime.now()
            context.last_ai_response = datetime.now()
            
            # Clear the pending message
            context.pending_intervention_message = None
            
            return intervention_message
        else:
            # Fallback to old method if no centralized message available
            print("‚ö†Ô∏è  No centralized intervention message found, using fallback")
            return await self._generate_research_based_response(context)
    
    async def _generate_research_based_response(self, context: ConversationContext) -> Optional[str]:
        """Generate response based on research guidelines with concurrency protection"""
        # Determine intervention type
        intervention_type = self._determine_intervention_type(context)
        
        print(f"üß† AI RESPONSE GENERATION:")
        print(f"   Room: {context.room_id}")
        print(f"   Intervention Type: {intervention_type.upper().replace('_', ' ')}")
        print(f"   Messages in Context: {len(context.messages)}")
        print(f"   Generation Lock: {context.ai_generating_response}")
        
        # Build conversation context
        conversation_text = ""
        for msg in context.messages[-8:]:
            conversation_text += f"{msg.username}: {msg.content}\n"
        
        # Create research-based system prompt
        system_prompt = self._create_research_prompt(context, intervention_type, conversation_text)
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Provide {intervention_type} intervention in 70 words or less."}
                ],
                max_tokens=85,
                temperature=0.7
            )
            
            ai_response = response.choices[0].message.content.strip()
            
            if ai_response == "SKIP_RESPONSE" or not ai_response:
                # Release lock if not sending response
                context.ai_generating_response = False
                context.ai_response_lock_time = None
                print(f"üîì AI RESPONSE LOCK: Released (no response) for room {context.room_id}")
                return None
            
            # Smart length adaptation for fallback responses too
            words = ai_response.split()
            word_count = len(words)
            
            if word_count > 15:
                if word_count > 30:
                    ai_response = ' '.join(words[:30]) + "..."
                    print(f"‚ö†Ô∏è  Fallback message truncated from {word_count} to 30 words")
                elif word_count > 20:
                    print(f"üìù Longer fallback message: {word_count} words")
            else:
                print(f"‚úÖ Concise fallback message: {word_count} words")
            
            # Update context based on intervention
            self._update_context_after_intervention(context, intervention_type)
            
            print(f"‚úÖ AI RESPONSE GENERATED: {len(ai_response.split())} words")
            print(f"   Preview: {ai_response[:100]}{'...' if len(ai_response) > 100 else ''}")
                
            return ai_response
            
        except Exception as e:
            # Release lock on error
            context.ai_generating_response = False
            context.ai_response_lock_time = None
            print(f"üîì AI RESPONSE LOCK: Released (error) for room {context.room_id}")
            print(f"‚ùå ERROR generating research-based response: {e}")
            return None
    
    def _determine_intervention_type(self, context: ConversationContext) -> str:
        """Determine which type of research-based intervention is needed"""
        # Check if we have an LLM decision from previous analysis
        if hasattr(context, 'llm_intervention_decision') and context.llm_intervention_decision:
            llm_decision = context.llm_intervention_decision
            
            # Map LLM decisions to intervention types
            decision_mapping = {
                "PROVIDE_HINT": "provide_hint",
                "PROVIDE_SOLUTION": "provide_solution", 
                "ENCOURAGE_PLANNING": "encourage_planning",
                "PROMPT_REFLECTION": "prompt_reflection",
                "ADDRESS_MISDIRECTION": "address_misdirection"
            }
            
            if llm_decision in decision_mapping:
                print(f"üß† USING LLM DECISION: {llm_decision} ‚Üí {decision_mapping[llm_decision]}")
                # Clear the decision after using it
                context.llm_intervention_decision = None
                return decision_mapping[llm_decision]
        
        # Fallback - should not be reached since centralized LLM handles all decisions
        print("‚ö†Ô∏è  Fallback intervention type - centralized LLM should handle all decisions")
        return "technical_assistance"
    
    # Misdirection detection removed - centralized LLM handles all conversation analysis intelligently
    
    def _create_research_prompt(self, context: ConversationContext, intervention_type: str, conversation_text: str) -> str:
        """Create research-based system prompt"""
        base_context = f"""You are CodeBot, an AI pair programming assistant following research-based pedagogical principles.

Context:
- Problem: {context.problem_title or "General coding"}
- Language: {context.programming_language}
- Code: {context.code_context[:400] if context.code_context else "No code visible"}

Recent conversation:
{conversation_text}
"""
        
        intervention_prompts = {
            "provide_hint": base_context + """
Your role: Technical Copilot - Provide Hints
Research guidance: "Do it with me instead of do it for me" - give hints rather than direct solutions to maintain code ownership and understanding.

Provide a helpful hint that guides them toward the solution without giving it away completely. Focus on the next step they should consider. KEEP IT VERY SHORT - PREFER 5-10 WORDS.""",
            
            "provide_solution": base_context + """
Your role: Technical Copilot - Provide Decomposed Solution  
Research guidance: When students are still stuck after hints, provide step-by-step solutions to ensure better code understanding.

Break down the solution into clear, digestible steps with explanations. BE CONCISE BUT COMPLETE - MAX 25 WORDS when step-by-step explanation is needed.""",
            
            "encourage_planning": base_context + """
Your role: Learning Facilitator - Encourage Planning
Research guidance: Novice programmers often don't do enough planning. Understanding the problem and having a plan is crucial.

Ask them to describe their approach in steps before coding. USE VERY SHORT QUESTIONS - PREFER 3-8 WORDS.""",
            
            "prompt_reflection": base_context + """
Your role: Learning Facilitator - Prompt Reflection
Research guidance: Reflection helps prevent overreliance on AI and ensures code understanding.

Ask them to walk through what their code does or explain their reasoning. ASK ONE FOCUSED QUESTION - PREFER 4-8 WORDS.""",
            
            "address_imbalance": base_context + """
Your role: Communication Facilitator - Address Imbalance
Research guidance: Skill mismatches can lead to one student dominating. Encourage balanced participation.

Gently encourage the quieter student to share thoughts or ask the dominant student to pause and summarize. BE VERY BRIEF - PREFER 5-10 WORDS.""",
            
            "address_misdirection": base_context + """
Your role: Communication Facilitator - Address Misdirection
Research guidance: When students discuss wrong ideas for ‚â•30s, redirect them back to the right direction to improve dialogue productivity.

Gently question their current approach and suggest they reconsider. Use format: "I think there might be some issue with [their idea], since it is [reason], what do you think?" """,
            
            "technical_assistance": base_context + """
Your role: Technical Copilot - General Assistance
Provide helpful technical guidance while maintaining their learning autonomy."""
        }
        
        return intervention_prompts.get(intervention_type, intervention_prompts["technical_assistance"])
    
    def _update_context_after_intervention(self, context: ConversationContext, intervention_type: str):
        """Update context state after intervention"""
        if intervention_type == "encourage_planning":
            context.has_planned = True
        elif intervention_type == "address_imbalance":
            context.dominant_user = None  # Reset dominance tracking
            context.user_participation = {}  # Reset participation counts
        # Topic tracking removed - no longer needed
        
        # Mark intervention as sent with new tracking system
        self._mark_intervention_sent(context, intervention_type)
            
    async def generate_speech(self, text: str) -> Optional[bytes]:
        """Generate speech audio from text using OpenAI TTS"""
        if not self.client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            response = self.client.audio.speech.create(
                model=self.voice_config["model"],
                voice=self.voice_config["voice"],
                input=text,
                speed=self.voice_config["speed"]
            )
            
            # Return the audio bytes
            return response.content
            
        except Exception as e:
            print(f"Error generating speech: {e}")
            return None

    async def generate_streaming_speech(self, text: str, room_id: str, message_id: str):
        """Generate streaming speech audio using OpenAI's streaming TTS API (following official docs)"""
        if not self.async_client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            # Signal start of streaming
            self.socketio.emit('ai_audio_stream_start', {
                'messageId': message_id,
                'room': room_id
            }, room=room_id, namespace='/ws')
            
            chunk_number = 0
            total_bytes_sent = 0
            
            print(f"üé§ Starting to stream audio for text: '{text[:50]}...'")
            
            # Use OpenAI's official streaming approach with AsyncOpenAI
            async with self.async_client.audio.speech.with_streaming_response.create(
                model=self.voice_config["model"],  # tts-1
                voice=self.voice_config["voice"],  # nova
                input=text,
                speed=self.voice_config["speed"],
                response_format="pcm"  # PCM for true real-time streaming
            ) as response:
                chunks_sent = []
                # Stream chunks directly as they arrive - TRUE real-time streaming
                async for chunk in response.iter_bytes(chunk_size=1024 * 2):  # 2KB chunks for PCM real-time
                    if chunk:
                        chunk_number += 1
                        total_bytes_sent += len(chunk)
                        chunk_base64 = base64.b64encode(chunk).decode('utf-8')
                        chunks_sent.append(chunk_number)
                        
                        print(f"üì¶ Streaming PCM chunk {chunk_number}: {len(chunk)} bytes (real-time)")
                        
                        # Send chunk immediately as it arrives from OpenAI
                        self.socketio.emit('ai_audio_chunk', {
                            'messageId': message_id,
                            'audioData': chunk_base64,
                            'chunkNumber': chunk_number,
                            'totalBytes': total_bytes_sent,
                            'room': room_id,
                            'isComplete': False,  # We don't know if this is the last chunk yet
                            'isRealtime': True,
                            'format': 'pcm'  # PCM format for true real-time streaming
                        }, room=room_id, namespace='/ws')
                        
                        # No artificial delay - stream as fast as data arrives from OpenAI

                
                # print(f"üèÅ Sending final chunk marker for chunk {final_chunk_number}")
                
                # Send a special "final chunk" marker
                if chunks_sent:
                    final_chunk_number = chunks_sent[-1]
                    print(f"üèÅ Sending final chunk marker for chunk {final_chunk_number}")
                    
                    # Send a special "final chunk" marker
                    self.socketio.emit('ai_audio_chunk', {
                        'messageId': message_id,
                        'audioData': '',  # Empty data
                        'chunkNumber': final_chunk_number,
                        'totalBytes': total_bytes_sent,
                        'room': room_id,
                        'isComplete': True,  # Mark as final
                        'isRealtime': True,
                        'format': 'pcm',
                        'isFinalMarker': True  # Special flag to indicate this is just a marker
                    }, room=room_id, namespace='/ws')

                # Signal completion - only that streaming is done, not that playback is done
                self.socketio.emit('ai_audio_complete', {
                    'messageId': message_id,
                    'room': room_id,
                    'totalChunks': chunk_number,
                    'totalBytes': total_bytes_sent,
                    'format': 'pcm'
                }, room=room_id, namespace='/ws')
                
                # DON'T send ai_audio_done here - let frontend determine when playback is actually finished
                
                print(f"‚úÖ Streamed {chunk_number} PCM chunks ({total_bytes_sent} bytes) in real-time for message {message_id}")
            return True
            
        except Exception as e:
            print(f"Error generating streaming speech: {e}")
            # Signal error
            self.socketio.emit('ai_audio_error', {
                'messageId': message_id,
                'room': room_id,
                'error': str(e)
            }, room=room_id, namespace='/ws')
            
            # CRITICAL: Even on error, signal that audio streaming is done
            self.socketio.emit('ai_audio_done', {
                'messageId': message_id,
                'room': room_id,
                'status': 'error'
            }, room=room_id, namespace='/ws')
            
            return None

    async def send_ai_message_with_audio(self, room_id: str, content: str):
        """Send an AI message to the chat room with streaming audio - optimized for speed"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': True,  # Will have streaming audio
            'isStreaming': True  # Indicate this is a streaming response
        }
        
        # Update last response time but DO NOT release generation lock yet
        # Lock will be released when audio playback is actually complete
        if room_id in self.conversation_history:
            context = self.conversation_history[room_id]
            context.last_ai_response = datetime.now()
            print(f"üîí AI RESPONSE LOCK: Keeping lock until audio playback complete for room {room_id}")
        
        # Send message immediately (don't wait for audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
        
        # Generate streaming audio in parallel (non-blocking)
        def generate_and_stream_audio():
            try:
                # Run the async audio streaming in a new event loop
                async def audio_task():
                    await self.generate_streaming_speech(content, room_id, message['id'])
                
                # Create and run new event loop for audio generation
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(audio_task())
                loop.close()
            except Exception as e:
                print(f"Error in streaming audio: {e}")
                # Fallback to simple notification that audio failed
                self.socketio.emit('ai_audio_error', {
                    'messageId': message['id'],
                    'room': room_id,
                    'error': 'Audio generation failed'
                }, room=room_id, namespace='/ws')
        
        # Start audio streaming in a separate thread
        threading.Thread(target=generate_and_stream_audio, daemon=True).start()
    
    def send_ai_message_text_only(self, room_id: str, content: str):
        """Send an AI message to the chat room without audio"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': False
        }
        
        # Update last response time and set pending state for non-greeting messages
        if room_id in self.conversation_history:
            self.conversation_history[room_id].last_ai_response = datetime.now()
            # Mark as pending response if it's an intervention (not a greeting)
            if not content.startswith(("Hi!", "Hello!", "Welcome!")):
                self.conversation_history[room_id].pending_ai_response = True
            
        # Emit the message to the room (no audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
    
    def send_ai_message(self, room_id: str, content: str):
        """Send an AI message to the chat room (optimized sync version)"""
        try:
            # Try to use existing event loop if available
            try:
                loop = asyncio.get_running_loop()
                asyncio.create_task(self.send_ai_message_with_audio(room_id, content))
                return
            except RuntimeError:
                # No running loop - use thread pool to avoid blocking
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.send_ai_message_with_audio(room_id, content))
                    return
        except Exception as e:
            # Fallback to simple text-only message
            self.send_ai_message_text_only(room_id, content)
        
    async def process_message(self, message_data: Dict[str, Any]):
        """Process a new message and potentially respond"""
        room_id = message_data.get('room')
        if not room_id:
            return
            
        # Add message to context
        self.add_message_to_context(message_data)
        

        # Check if we should respond
        if self.should_respond(room_id):
            print("AI will respond to the message")
            # Generate response asynchronously
            response = await self.generate_response(room_id)
            
            if response:
                # Send the response using async version
                await self.send_ai_message_with_audio(room_id, response)
                
    def process_message_sync(self, message_data: Dict[str, Any]):
        """Synchronous wrapper for processing messages"""
        try:
            # Run the async function in a new event loop
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(self.process_message(message_data))
            loop.close()
        except Exception as e:
            print(f"Error processing message in AI agent: {e}")
            # Continue processing other messages
            
    def handle_code_update(self, room_id: str, code: str, language: str = "python"):
        """Handle code updates from the editor"""
        self.update_code_context(room_id, code, language)
        
    def handle_problem_update(self, room_id: str, problem_title: str, problem_description: str):
        """Handle problem description updates"""
        self.update_problem_context(room_id, problem_title, problem_description)
        
    def release_generation_lock(self, room_id: str, message_id: str = None):
        """Release AI generation lock when audio playback is complete"""
        if room_id in self.conversation_history:
            context = self.conversation_history[room_id]
            if context.ai_generating_response:
                context.ai_generating_response = False
                context.ai_response_lock_time = None
                print(f"üîì AI RESPONSE LOCK: Released after audio playback complete for room {room_id} (message: {message_id})")
            else:
                print(f"üîÑ AI RESPONSE LOCK: Already released for room {room_id}")
        else:
            print(f"‚ö†Ô∏è AI RESPONSE LOCK: No context found for room {room_id}")
    
    def join_room(self, room_id: str):
        """AI agent joins a room"""
        # Only send greeting if OpenAI client is available
        if not self.client:
            print(f"‚ö†Ô∏è  CodeBot cannot join room {room_id}: OpenAI client not initialized")
            return
            
        # Send a greeting message when joining
        greeting_messages = [
            # "Hi! I'm CodeBot, your AI pair programming assistant. I'll help with technical questions, encourage good planning, and facilitate your collaboration. Let's code together!",
            # "Hello! I'm here to support your pair programming session. I can provide hints when you're stuck, help with code review, and ensure both of you stay engaged. Ready to start?",
            "Welcome! I'm CodeBot, designed to enhance your pair programming experience. I'll offer technical guidance and help maintain productive collaboration."
        ]
        
        import random
        greeting = random.choice(greeting_messages)
        
        # Send greeting after a short delay (non-blocking)
        def send_greeting():
            time.sleep(2)  # Wait 2 seconds before greeting
            self.send_ai_message_text_only(room_id, greeting)
            
        # Start greeting in a separate thread to avoid blocking
        threading.Thread(target=send_greeting, daemon=True).start()

    def set_voice_config(self, voice: str = None, model: str = None, speed: float = None):
        """Update voice configuration for TTS"""
        if voice and voice in ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]:
            self.voice_config["voice"] = voice
        if model and model in ["tts-1", "tts-1-hd"]:
            self.voice_config["model"] = model
        if speed and 0.25 <= speed <= 4.0:
            self.voice_config["speed"] = speed

    def _start_research_based_monitoring(self):
        """Start background monitoring based on research interventions"""
        def monitor_conversations():
            while True:
                try:
                    self._monitor_silence_and_errors()
                    self._monitor_code_reviews()
                    time.sleep(5)  # Check every 5 seconds for responsiveness
                except Exception as e:
                    print(f"Error in research-based monitoring: {e}")
                    time.sleep(5)
        
        threading.Thread(target=monitor_conversations, daemon=True).start()
    
    def _should_intervene_based_on_research(self, context: ConversationContext) -> tuple[bool, str]:
        """PRIMARY intervention logic based on research findings"""
        if not context.messages:
            print("üö´ AI WILL NOT INTERVENE: No conversation history")
            return False, ""
        
        # Get recent messages (last 3)
        recent_messages = context.messages[-3:]
        discussion_duration = self._get_discussion_duration_since_last_intervention(context)
        
        print(f"\nü§ñ AI Agent: Evaluating intervention need...")
        print(f"   Discussion Duration: {discussion_duration:.1f}s")
        print(f"   Last AI Intervention: {self.last_ai_intervention_time}")
        print(f"   Recent Messages: {len(recent_messages)}")
        
        # STEP 1: 30-SECOND THRESHOLD (CODE DECISION, NOT LLM)
        if discussion_duration < 30:
            print(f"üö´ AI WILL NOT INTERVENE: 30s threshold not met ({discussion_duration:.1f}s < 30s)")
            print(f"   Waiting {30 - discussion_duration:.1f}s more before any intervention possible")
            return False, ""
        
        print(f"‚úì 30s threshold met ({discussion_duration:.1f}s), proceeding with evaluation...")
        
        # STEP 2: ONLY CHECK CRITICAL THRESHOLDS (silence/errors)
        # Check for prolonged silence - this is a clear intervention trigger
        if hasattr(context, 'silence_start') and context.silence_start:
            silence_duration = (datetime.now() - context.silence_start).total_seconds()
            if silence_duration >= 30:  # 30+ seconds of silence
                print(f"‚úÖ AI WILL INTERVENE: Prolonged silence ({silence_duration:.1f}s)")
                return True, "Stuck?"
        
        # Check for repeated errors - clear intervention trigger
        if context.consecutive_errors >= 3:
            print(f"‚úÖ AI WILL INTERVENE: Multiple consecutive errors ({context.consecutive_errors})")
            return True, "Same error. Help?"
        
        # STEP 3: LET AI MAKE ALL OTHER DECISIONS (CENTRALIZED LLM CALL)
        print(f"üß† Using AI to analyze conversation and make intervention decision...")
        should_intervene, intervention_message = self._centralized_llm_intervention_decision(context, discussion_duration)
        
        return should_intervene, intervention_message
        
    def _get_discussion_duration_since_last_intervention(self, context: ConversationContext) -> float:
        """Calculate how long discussion has been going since last AI intervention"""
        if not hasattr(self, 'last_ai_intervention_time') or not self.last_ai_intervention_time:
            # If no previous intervention, consider discussion has been going for a while
            return 35.0  # Default to 35 seconds to pass the 30s threshold
        
        current_time = datetime.now()
        duration = (current_time - self.last_ai_intervention_time).total_seconds()
        return duration
    
    def _centralized_llm_intervention_decision(self, context: ConversationContext, discussion_duration: float) -> tuple[bool, str]:
        """CENTRALIZED LLM CALL: Decide intervention + type + generate message ALL IN ONE"""
        if not self.client:
            # If no LLM available, don't intervene (except for silence/errors which are handled above)
            print("üö´ AI WILL NOT INTERVENE: No LLM client available")
            return False, ""
        
        # Get recent conversation context (last 5 messages)
        recent_conversation = ""
        for msg in context.messages[-5:]:
            recent_conversation += f"{msg.username}: {msg.content}\n"
        
        # SINGLE comprehensive prompt that handles EVERYTHING
        comprehensive_prompt = f"""You are CodeBot, an AI pair programming assistant. Analyze this conversation and decide:
1. Should you intervene? (The 30+ second threshold has been met)
2. If yes, what type of intervention?
3. What message should you send? (Max 70 words)

Problem Context:
- Problem: {context.problem_title or "General coding"}
- Language: {context.programming_language}
- Discussion Duration: {discussion_duration:.1f} seconds

Recent Conversation:
{recent_conversation}

Code Context:
{context.code_context[:300] if context.code_context else "No code visible"}

RESEARCH-BASED INTERVENTION TYPES:
1. Technical Copilot - Provide hints/solutions when stuck
2. Learning Facilitator - Encourage planning/reflection  
3. Communication Facilitator - Address imbalance/misdirection

USE YOUR INTELLIGENCE TO EVALUATE:
- Are they actively discussing and making progress? ‚Üí NO_INTERVENTION
- Are they stuck, confused, or asking for help? ‚Üí PROVIDE_HINT/PROVIDE_SOLUTION
- Are they jumping into code without planning? ‚Üí ENCOURAGE_PLANNING
- Have they completed something without explaining? ‚Üí PROMPT_REFLECTION
- Is one person dominating the conversation? ‚Üí ADDRESS_IMBALANCE
- Are they discussing wrong approaches for too long? ‚Üí ADDRESS_MISDIRECTION

RESPONSE FORMAT:
If intervention needed: "INTERVENE|[TYPE]|[MESSAGE]"
If no intervention: "NO_INTERVENTION"

Types: PROVIDE_HINT, PROVIDE_SOLUTION, ENCOURAGE_PLANNING, PROMPT_REFLECTION, ADDRESS_MISDIRECTION, ADDRESS_IMBALANCE

KEEP MESSAGES EXTREMELY CONCISE:
- PREFER 5-15 words for most situations
- MAX 30 words only when explanation is absolutely critical
- Use short, direct phrases and questions
- Avoid explanations unless solving complex problems
- Examples: "Stuck?" "What's your approach?" "Try the base case." "Need help with that loop?"

WHEN TO USE LONGER MESSAGES (20-30 words):
- Providing step-by-step solutions for complex problems
- Addressing serious misdirection that needs explanation
- Multiple consecutive errors requiring detailed debugging help

Your response (PREFER 5-15 words, MAX 30 when critical):"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are CodeBot, an expert pair programming facilitator. Make ONE decision covering intervention, type, and message. PRIORITIZE VERY SHORT MESSAGES (5-15 words) unless complex explanation is truly needed."},
                    {"role": "user", "content": comprehensive_prompt}
                ],
                max_tokens=120,  # Enough for decision + 70-word message
                temperature=0.7
            )
            
            llm_response = response.choices[0].message.content.strip()
            
            if llm_response == "NO_INTERVENTION" or llm_response.startswith("NO_INTERVENTION"):
                print(f"üö´ AI WILL NOT INTERVENE: LLM Decision - Continue discussion ({discussion_duration:.1f}s)")
                print(f"   Reason: Users are discussing productively, let them continue")
                return False, ""
            
            elif llm_response.startswith("INTERVENE|"):
                # Parse the structured response: INTERVENE|TYPE|MESSAGE
                parts = llm_response.split("|", 2)
                if len(parts) >= 3:
                    intervention_type = parts[1]
                    intervention_message = parts[2]
                    
                    # Smart length adaptation: prefer very short, allow longer when critical
                    words = intervention_message.split()
                    word_count = len(words)
                    
                    # Most messages should be 5-15 words
                    if word_count > 15:
                        # Allow up to 30 words for complex situations, but warn if excessive
                        if word_count > 30:
                            intervention_message = ' '.join(words[:30]) + "..."
                            print(f"‚ö†Ô∏è  Message truncated from {word_count} to 30 words")
                        elif word_count > 20:
                            print(f"üìù Longer message used: {word_count} words (acceptable for complex situation)")
                    else:
                        print(f"‚úÖ Concise message: {word_count} words (preferred length)")
                    
                    print(f"‚úÖ AI WILL INTERVENE: LLM Centralized Decision")
                    print(f"   Discussion Duration: {discussion_duration:.1f}s")
                    print(f"   Intervention Type: {intervention_type}")
                    print(f"   Message Preview: {intervention_message[:50]}{'...' if len(intervention_message) > 50 else ''}")
                    
                    # Update context for tracking
                    self._update_context_after_intervention(context, intervention_type.lower())
                    
                    return True, intervention_message
                else:
                    print(f"‚ùå LLM response format error: {llm_response}")
                    return False, ""
            else:
                print(f"‚ùå Unexpected LLM response format: {llm_response}")
                return False, ""
                
        except Exception as e:
            print(f"‚ùå Error in centralized LLM intervention: {e}")
            # Conservative fallback - don't interrupt
            print(f"üö´ AI WILL NOT INTERVENE: LLM error, defaulting to no intervention")
            return False, ""
    
    def _detect_help_request(self, messages: List[Message]) -> bool:
        """Use LLM to intelligently detect when students need technical help"""
        if not self.client:
            # Fallback: Only look for explicit question marks if no LLM
            return any('?' in message.content for message in messages)
        
        # Build recent conversation context
        conversation_text = ""
        for msg in messages:
            conversation_text += f"{msg.username}: {msg.content}\n"
        
        help_detection_prompt = f"""Analyze this pair programming conversation to determine if the students need immediate technical help.

Recent conversation:
{conversation_text}

Look for:
- Direct requests for help ("I'm stuck", "Can you help", "How do we...")
- Technical confusion or errors being discussed
- Questions about code, syntax, or implementation
- Expressions of being lost or not understanding

Respond with ONLY:
- "HELP_NEEDED" - They are asking for immediate technical assistance
- "NO_HELP_NEEDED" - They are discussing normally without needing intervention"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are an expert at detecting when students need technical help in pair programming."},
                    {"role": "user", "content": help_detection_prompt}
                ],
                max_tokens=20,
                temperature=0.1
            )
            
            help_decision = response.choices[0].message.content.strip()
            is_help_needed = help_decision == "HELP_NEEDED"
            
            if is_help_needed:
                print(f"üß† LLM HELP DETECTION: HELP_NEEDED - Direct technical assistance requested")
            else:
                print(f"üß† LLM HELP DETECTION: NO_HELP_NEEDED - Normal discussion continuing")
            
            return is_help_needed
            
        except Exception as e:
            print(f"‚ùå Error in LLM help detection: {e}")
            # Conservative fallback - only respond to explicit questions
            return any('?' in message.content for message in messages)
    
    def _should_encourage_planning(self, context: ConversationContext, messages: List[Message]) -> bool:
        """Research: Encourage planning before coding"""
        if context.has_planned:
            return False
        
        # Check if they're jumping into coding without planning
        coding_indicators = ['def ', 'function', 'class ', 'import ', 'for ', 'while ', 'if ']
        planning_indicators = ['plan', 'approach', 'steps', 'strategy', 'algorithm', 'pseudocode']
        
        has_coding = False
        has_planning = False
        
        for message in messages:
            content_lower = message.content.lower()
            if any(indicator in content_lower for indicator in coding_indicators):
                has_coding = True
            if any(indicator in content_lower for indicator in planning_indicators):
                has_planning = True
        
        # If they're coding without planning, suggest planning
        return has_coding and not has_planning
    
    def _should_prompt_reflection(self, context: ConversationContext, messages: List[Message]) -> bool:
        """Research: Prompt reflection to assess understanding"""
        if not context.code_context:
            return False
        
        # Check if they've made significant progress without reflecting
        progress_indicators = ['working', 'done', 'finished', 'complete', 'solved']
        
        for message in messages:
            content_lower = message.content.lower()
            if any(indicator in content_lower for indicator in progress_indicators):
                return True  # They think they're done, time for reflection
        
        return False
    
    def _check_user_dominance(self, context: ConversationContext):
        """Research: Detect and handle skill imbalance"""
        if len(context.user_participation) < 2:
            return
        
        total_messages = sum(context.user_participation.values())
        if total_messages < 6:  # Need enough data
            return
        
        # Find most active user
        most_active_user = max(context.user_participation.items(), key=lambda x: x[1])
        dominance_ratio = most_active_user[1] / total_messages
        
        if dominance_ratio > self.dominance_threshold:
            context.dominant_user = most_active_user[0]
    
    def _track_error_patterns(self, context: ConversationContext, message: Message):
        """Research: Track consecutive identical errors"""
        content_lower = message.content.lower()
        error_keywords = ['error', 'exception', 'traceback', 'failed', 'syntax error']
        
        if any(keyword in content_lower for keyword in error_keywords):
            current_time = datetime.now()
            if (context.last_error_time and 
                (current_time - context.last_error_time).total_seconds() < 60):  # Within 1 minute
                context.consecutive_errors += 1
            else:
                context.consecutive_errors = 1
            context.last_error_time = current_time
        else:
            # Reset error count on non-error message
            context.consecutive_errors = 0
    
    # Topic tracking removed - LLM handles all conversation analysis intelligently
    
    def _monitor_silence_and_errors(self):
        """Monitor for silence ‚â•30s or ‚â•3 identical errors (simplified, no topic tracking)"""
        current_time = datetime.now()
        
        for room_id, context in self.conversation_history.items():
            if not context.messages:
                continue
            
            # Skip monitoring if AI is waiting for a response
            if context.pending_ai_response:
                continue
            
            # Check for silence (‚â•30 seconds)
            if context.last_activity_time:
                silence_duration = (current_time - context.last_activity_time).total_seconds()
                
                # Only intervene for silence if it's been long enough and we haven't recently
                if silence_duration >= self.silence_threshold:
                    print(f"üîç BACKGROUND MONITORING: Silence detected in room {room_id} ({silence_duration:.1f}s ‚â• {self.silence_threshold}s)")
                    print(f"   ‚úÖ WILL INTERVENE: Calling silence intervention")
                    self._intervene_for_silence(room_id, context)
                else:
                    # Periodically log that we're monitoring but not intervening yet
                    if int(silence_duration) % 10 == 0 and silence_duration > 10:  # Every 10 seconds after 10s
                        print(f"üîç BACKGROUND MONITORING: Silence in room {room_id} ({silence_duration:.1f}s < {self.silence_threshold}s threshold)")
                        print(f"   üö´ WILL NOT INTERVENE: Waiting for {self.silence_threshold}s threshold")
            
            # Check for consecutive errors (‚â•3)
            if context.consecutive_errors >= self.error_threshold:
                print(f"üîç BACKGROUND MONITORING: Repeated errors detected in room {room_id} ({context.consecutive_errors} ‚â• {self.error_threshold})")
                print(f"   ‚úÖ WILL INTERVENE: Calling error pattern intervention")
                self._intervene_for_repeated_errors(room_id, context)
                context.consecutive_errors = 0  # Reset after intervention
            elif context.consecutive_errors > 0:
                print(f"üîç BACKGROUND MONITORING: Error count in room {room_id} ({context.consecutive_errors} < {self.error_threshold} threshold)")
                print(f"   üö´ WILL NOT INTERVENE: Waiting for {self.error_threshold} consecutive errors")
    
    def _monitor_code_reviews(self):
        """Research: Issue check after each code block (every 5s)"""
        current_time = datetime.now()
        
        for room_id, context in self.conversation_history.items():
            if not context.code_context:
                continue
            
            # Check if code review is needed
            time_since_review = float('inf')
            if context.last_code_review_time:
                time_since_review = (current_time - context.last_code_review_time).total_seconds()
            
            if time_since_review >= self.code_review_interval:
                print(f"üîç MONITORING: Code review needed in room {room_id} (time since last: {time_since_review:.1f}s ‚â• {self.code_review_interval}s)")
                self._perform_code_review(room_id, context)
                context.last_code_review_time = current_time
    
    def _mark_intervention_sent(self, context: ConversationContext, intervention_type: str):
        """Mark that an intervention was sent to track cooldowns"""
        current_time = datetime.now()
        context.interventions_sent[intervention_type] = current_time
        context.pending_ai_response = True  # AI is now waiting for user response
        context.last_ai_response = current_time
        context.intervention_escalation_level += 1
        
        # Release generation lock after marking intervention
        context.ai_generating_response = False
        context.ai_response_lock_time = None
        
        # Console logging for debugging and monitoring
        print(f"ü§ñ AI INTERVENTION: {intervention_type.upper().replace('_', ' ')}")
        print(f"   Room: {context.room_id}")
        print(f"   Escalation Level: {context.intervention_escalation_level}")
        print(f"   Pending Response: {context.pending_ai_response}")
        print(f"   Time: {current_time.strftime('%H:%M:%S')}")
        print(f"üîì AI INTERVENTION LOCK: Released after sending {intervention_type}")
    
    def _intervene_for_silence(self, room_id: str, context: ConversationContext):
        """Research: Intervene when silence ‚â•30s (smart, non-annoying version)"""
        if not self._should_send_intervention(context, 'silence'):
            print(f"üö´ SILENCE INTERVENTION BLOCKED: Cooldown or other constraint for room {room_id}")
            return
        
        print(f"‚úÖ SENDING SILENCE INTERVENTION: Room {room_id}, escalation level {context.intervention_escalation_level}")
        
        # Escalate messages based on how many times we've intervened (VERY CONCISE)
        messages_by_level = [
            # First attempt (gentle, 2-4 words)
            [
                "Thoughts?",
                "Ideas?", 
                "Stuck?"
            ],
            # Second attempt (more direct, 3-6 words)
            [
                "Need help?",
                "Want a hint?",
                "Discuss your approach?"
            ],
            # Third attempt (specific help, 4-8 words)
            [
                "Ready for guidance?",
                "Want me to help?",
                "Need a solution approach?"
            ]
        ]
        
        level = min(context.intervention_escalation_level, len(messages_by_level) - 1)
        import random
        message = random.choice(messages_by_level[level])
        
        print(f"   Message: {message}")
        
        self.send_ai_message_text_only(room_id, message)
        self._mark_intervention_sent(context, 'silence')
    
    def _intervene_for_repeated_errors(self, room_id: str, context: ConversationContext):
        """Research: Intervene when ‚â•3 identical errors (smart, non-annoying version)"""
        if not self._should_send_intervention(context, 'repeated_errors'):
            print(f"üö´ ERROR INTERVENTION BLOCKED: Cooldown or other constraint for room {room_id}")
            return
        
        print(f"‚úÖ SENDING ERROR INTERVENTION: Room {room_id}, escalation level {context.intervention_escalation_level}")
        
        # Provide hints first, escalate to solutions (VERY CONCISE)
        messages_by_level = [
            "Same error. Hint?",
            "Error persists. Help?", 
            "Walk through solution?"
        ]
        
        level = min(context.intervention_escalation_level, len(messages_by_level) - 1)
        message = messages_by_level[level]
        
        print(f"   Message: {message}")
        
        self.send_ai_message_text_only(room_id, message)
        self._mark_intervention_sent(context, 'repeated_errors')
    
    def _perform_code_review(self, room_id: str, context: ConversationContext):
        """Research: Issue check after each code block (smart, non-annoying version)"""
        if not self._should_send_intervention(context, 'code_review'):
            return
        
        # Only if substantial code and not too frequent
        if len(context.code_context) > 50:  # Only if substantial code
            message = "Navigator: Take a moment to review the current code. Do you see any potential issues?"
            self.send_ai_message_text_only(room_id, message)
            self._mark_intervention_sent(context, 'code_review')
    
    def _should_send_intervention(self, context: ConversationContext, intervention_type: str = None) -> bool:
        """Smart intervention logic to prevent AI from being annoying"""
        # Rule 0: Don't send if AI is already generating a response (prevents concurrent responses)
        if context.ai_generating_response:
            if intervention_type:
                print(f"üö´ BLOCKED INTERVENTION: {intervention_type.upper().replace('_', ' ')} - AI currently generating response")
            return False
        
        # Rule 1: Don't send if AI is already waiting for a response
        if context.pending_ai_response:
            if intervention_type:
                print(f"üö´ BLOCKED INTERVENTION: {intervention_type.upper().replace('_', ' ')} - AI waiting for user response")
            return False
        
        # Rule 2: Check basic cooldown period
        if context.last_ai_response:
            time_since_last = datetime.now() - context.last_ai_response
            if time_since_last.total_seconds() < self.response_cooldown:
                if intervention_type:
                    print(f"üö´ BLOCKED INTERVENTION: {intervention_type.upper().replace('_', ' ')} - Basic cooldown active ({time_since_last.total_seconds():.1f}s < {self.response_cooldown}s)")
                return False
        
        # Rule 3: Check intervention-specific cooldown (prevent repeated same interventions)
        if intervention_type and intervention_type in context.interventions_sent:
            last_sent = context.interventions_sent[intervention_type]
            time_since_intervention = datetime.now() - last_sent
            
            # Different interventions have different cooldown periods
            intervention_cooldowns = {
                'silence': 300,  # 5 minutes before repeating silence intervention
                'misdirection': 180,  # 3 minutes before repeating misdirection intervention  
                'repeated_errors': 120,  # 2 minutes before repeating error intervention
                'code_review': 300,  # 5 minutes before repeating code review
                'planning': 900,  # 15 minutes before encouraging planning again
                'reflection': 600,  # 10 minutes before prompting reflection again
                'imbalance': 240,  # 4 minutes before addressing imbalance again
            }
            
            required_cooldown = intervention_cooldowns.get(intervention_type, 180)  # Default 3 minutes
            
            if time_since_intervention.total_seconds() < required_cooldown:
                print(f"üö´ BLOCKED INTERVENTION: {intervention_type.upper().replace('_', ' ')} - Specific cooldown active ({time_since_intervention.total_seconds():.1f}s < {required_cooldown}s)")
                return False
        
        # Rule 4: Escalation logic - be less frequent with higher escalation levels
        if context.intervention_escalation_level > 0:
            # Increase cooldown period for repeated interventions
            extended_cooldown = self.response_cooldown * (1 + context.intervention_escalation_level)
            if context.last_ai_response:
                time_since_last = datetime.now() - context.last_ai_response
                if time_since_last.total_seconds() < extended_cooldown:
                    if intervention_type:
                        print(f"üö´ BLOCKED INTERVENTION: {intervention_type.upper().replace('_', ' ')} - Escalation cooldown active ({time_since_last.total_seconds():.1f}s < {extended_cooldown}s)")
                    return False
        
        # Intervention approved - acquire lock for intervention
        if intervention_type:
            print(f"‚úÖ INTERVENTION APPROVED: {intervention_type.upper().replace('_', ' ')}")
            context.ai_generating_response = True
            context.ai_response_lock_time = datetime.now()
            print(f"üîí AI INTERVENTION LOCK: Acquired for {intervention_type}")
        
        return True
    
    # Misdirection detection removed - centralized LLM handles all conversation analysis intelligently

    def get_intervention_status(self, room_id: str) -> Dict[str, Any]:
        """Get current intervention status for debugging/monitoring"""
        if room_id not in self.conversation_history:
            return {"error": "Room not found"}
        
        context = self.conversation_history[room_id]
        current_time = datetime.now()
        
        status = {
            "pending_ai_response": context.pending_ai_response,
            "escalation_level": context.intervention_escalation_level,
            "interventions_sent": {},
            "time_since_last_ai_response": None,
            "time_since_last_user_response": None,
        }
        
        # Calculate time since interventions
        for intervention_type, last_sent in context.interventions_sent.items():
            time_since = (current_time - last_sent).total_seconds()
            status["interventions_sent"][intervention_type] = {
                "last_sent": last_sent.isoformat(),
                "time_since_seconds": time_since
            }
        
        if context.last_ai_response:
            status["time_since_last_ai_response"] = (current_time - context.last_ai_response).total_seconds()
        
        if context.last_user_response_time:
            status["time_since_last_user_response"] = (current_time - context.last_user_response_time).total_seconds()
        
        return status

# Global AI agent instance
ai_agent = None

def init_ai_agent(socketio_instance):
    """Initialize the AI agent"""
    global ai_agent
    ai_agent = AIAgent(socketio_instance)
    return ai_agent

def get_ai_agent():
    """Get the global AI agent instance"""
    return ai_agent
