"""
AI Agent Service - A proactive AI teammate for pair programming
Uses GPT-4o-mini to analyze conversations and provide helpful insights
"""

import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from openai import OpenAI, AsyncOpenAI
from dataclasses import dataclass
import threading
import time
import base64

@dataclass
class Message:
    id: str
    content: str
    username: str
    userId: str
    timestamp: str
    room: str
    isAutoGenerated: bool = False

@dataclass
class ConversationContext:
    messages: List[Message]
    room_id: str
    last_ai_response: Optional[datetime] = None
    code_context: str = ""
    programming_language: str = "python"
    problem_description: str = ""
    problem_title: str = ""

class AIAgent:
    def __init__(self, socketio_instance):
        # Check if OpenAI API key is available
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("‚ö†Ô∏è  Warning: No OpenAI API key found. AI agent will be disabled.")
            print("   Set OPENAI_API_KEY in your .env file to enable AI responses.")
            self.client = None
        else:
            try:
                self.client = OpenAI(api_key=api_key)
                self.async_client = AsyncOpenAI(api_key=api_key)  # For streaming TTS
                print("‚úÖ AI Agent (CodeBot) initialized successfully!")
            except Exception as e:
                print(f"‚ùå Error initializing OpenAI client: {e}")
                print("   AI agent will be disabled.")
                self.client = None
                self.async_client = None
        
        self.socketio = socketio_instance
        self.conversation_history = {}  # room_id -> ConversationContext
        self.response_cooldown = 15  # Minimum seconds between AI responses
        self.min_messages_before_response = 3  # Wait for at least 3 messages before responding
        self.max_context_messages = 10  # Keep last 10 messages for context
        
        # AI Agent identity and voice configuration
        self.agent_name = "CodeBot"
        self.agent_id = "ai_agent_codebot"
        self.voice_config = {
            "model": "tts-1",            # Use OpenAI's fast TTS model (tts-1 or tts-1-hd)
            "voice": "nova",             # Available: alloy, echo, fable, onyx, nova, shimmer
            "speed": 1.0                 # 0.25 to 4.0
        }
        

        
    def add_message_to_context(self, message_data: Dict[str, Any]):
        """Add a new message to the conversation context"""
        room_id = message_data.get('room')
        if not room_id:
            return
            
        # Skip AI's own messages
        if message_data.get('userId') == self.agent_id:
            return
            
        message = Message(
            id=message_data.get('id', ''),
            content=message_data.get('content', ''),
            username=message_data.get('username', 'Unknown'),
            userId=message_data.get('userId', ''),
            timestamp=message_data.get('timestamp', ''),
            room=room_id,
            isAutoGenerated=message_data.get('isAutoGenerated', False)
        )
        
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.messages.append(message)
        
        # Keep only the most recent messages
        if len(context.messages) > self.max_context_messages:
            context.messages = context.messages[-self.max_context_messages:]
            
    def update_code_context(self, room_id: str, code: str, language: str = "python"):
        """Update the current code context for a room"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.code_context = code
        context.programming_language = language
        
    def update_problem_context(self, room_id: str, problem_title: str, problem_description: str):
        """Update the current problem description for a room"""
        if room_id not in self.conversation_history:
            self.conversation_history[room_id] = ConversationContext(
                messages=[],
                room_id=room_id
            )
            
        context = self.conversation_history[room_id]
        context.problem_title = problem_title
        context.problem_description = problem_description

    def should_respond(self, room_id: str) -> bool:
        """Determine if the AI should respond based on various factors"""
        if room_id not in self.conversation_history:
            return False
            
        context = self.conversation_history[room_id]

        # Check cooldown period
        if context.last_ai_response:
            time_since_last = datetime.now() - context.last_ai_response
            if time_since_last < timedelta(seconds=self.response_cooldown):
                return False
                
        # Need minimum number of messages
        if len(context.messages) < self.min_messages_before_response:
            return False
            
        # Check if recent messages warrant a response
        recent_messages = context.messages[-3:]  # Last 3 messages
        
        # Keywords that might indicate need for help
        help_keywords = [
            'error', 'bug', 'problem', 'issue', 'stuck', 'help', 'question',
            'why', 'how', 'what', 'debug', 'fix', 'broken', 'fail', 'exception',
            'syntax', 'logic', 'algorithm', 'optimize', 'improve', 'better way',
            'confused', 'understand', 'explain', 'clarify'
        ]
        
        # Programming-related keywords
        programming_keywords = [
            'function', 'class', 'method', 'variable', 'loop', 'condition',
            'array', 'list', 'dict', 'object', 'import', 'library', 'module',
            'api', 'database', 'query', 'test', 'unittest', 'framework'
        ]
        
        for message in recent_messages:
            content_lower = message.content.lower()
            
            # Direct questions or help requests
            if any(keyword in content_lower for keyword in help_keywords):
                return True
                
            # Programming discussions
            if any(keyword in content_lower for keyword in programming_keywords):
                return True
                
            # Questions (contain question marks)
            if '?' in message.content:
                return True
                
        return False
        
    async def generate_response(self, room_id: str) -> Optional[str]:
        """Generate an AI response based on conversation context"""
        if not self.client:
            print("‚ö†Ô∏è  Cannot generate AI response: OpenAI client not initialized")
            return None
            
        if room_id not in self.conversation_history:
            return None
            
        context = self.conversation_history[room_id]
        
        # Build conversation history for the prompt
        conversation_text = ""
        for msg in context.messages[-5:]:  # Last 5 messages
            conversation_text += f"{msg.username}: {msg.content}\n"

        # Create the system prompt (optimized for speed)
        system_prompt = f"""You are CodeBot, an AI pair programming assistant.

                        Problem: {context.problem_title or "General coding"}
                        Description: {context.problem_description[:200] if context.problem_description else "None"}
                        Language: {context.programming_language}
                        Code: {context.code_context[:300] if context.code_context else "None"}

                        Recent chat:
                        {conversation_text}

                        Rules: Be helpful, concise (max 70 words), respond only when valuable. Skip with 'SKIP_RESPONSE' if not needed."""

        try:
            print(system_prompt)
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Respond in 70 words max or say 'SKIP_RESPONSE'."}
                ],
                max_tokens=80,  # Reduced to ensure ~70 words max
                temperature=0.7
            )
            
            ai_response = response.choices[0].message.content.strip()
            
            # Check if AI decided not to respond
            if ai_response == "SKIP_RESPONSE" or not ai_response:
                print("‚ö†Ô∏è  AI chose not to respond")
                return None
            
            # Ensure response doesn't exceed 70 words
            words = ai_response.split()
            if len(words) > 70:
                ai_response = ' '.join(words[:70]) + "..."
                
            return ai_response
            
        except Exception as e:
            print(f"Error generating AI response: {e}")
            return None
            
    async def generate_speech(self, text: str) -> Optional[bytes]:
        """Generate speech audio from text using OpenAI TTS"""
        if not self.client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            response = self.client.audio.speech.create(
                model=self.voice_config["model"],
                voice=self.voice_config["voice"],
                input=text,
                speed=self.voice_config["speed"]
            )
            
            # Return the audio bytes
            return response.content
            
        except Exception as e:
            print(f"Error generating speech: {e}")
            return None

    async def generate_streaming_speech(self, text: str, room_id: str, message_id: str):
        """Generate streaming speech audio using OpenAI's streaming TTS API (following official docs)"""
        if not self.async_client:
            return None
            
        try:
            # Limit text length to avoid very long audio files (70 words ‚âà 350-500 chars)
            if len(text) > 500:
                text = text[:500] + "..."
            
            # Signal start of streaming
            self.socketio.emit('ai_audio_stream_start', {
                'messageId': message_id,
                'room': room_id
            }, room=room_id, namespace='/ws')
            
            chunk_number = 0
            total_bytes_sent = 0
            
            print(f"üé§ Starting to stream audio for text: '{text[:50]}...'")
            
            # Use OpenAI's official streaming approach with AsyncOpenAI
            async with self.async_client.audio.speech.with_streaming_response.create(
                model=self.voice_config["model"],  # tts-1
                voice=self.voice_config["voice"],  # nova
                input=text,
                speed=self.voice_config["speed"],
                response_format="pcm"  # PCM for true real-time streaming
            ) as response:
                
                # Stream audio chunks as they're generated (true real-time)
                chunks_sent = []
                async for chunk in response.iter_bytes(chunk_size=1024 * 2):  # 2KB chunks for PCM real-time
                    if chunk:
                        chunk_number += 1
                        total_bytes_sent += len(chunk)
                        chunks_sent.append(chunk_number)
                        chunk_base64 = base64.b64encode(chunk).decode('utf-8')
                        
                        print(f"üì¶ Sending PCM chunk {chunk_number}: {len(chunk)} bytes")
                        
                        # Emit real-time audio chunk (we don't know if it's final yet)
                        self.socketio.emit('ai_audio_chunk', {
                            'messageId': message_id,
                            'audioData': chunk_base64,
                            'chunkNumber': chunk_number,
                            'totalBytes': total_bytes_sent,
                            'room': room_id,
                            'isComplete': False,  # We don't know yet if this is final
                            'isRealtime': True,
                            'format': 'pcm'  # PCM format for true real-time streaming
                        }, room=room_id, namespace='/ws')
                        
                        # No artificial delay - stream as fast as data arrives
                
                # Now we know the total number of chunks - send final chunk marker
                if chunks_sent:
                    final_chunk_number = chunks_sent[-1]
                    print(f"üèÅ Sending final chunk marker for chunk {final_chunk_number}")
                    
                    # Send a special "final chunk" marker
                    self.socketio.emit('ai_audio_chunk', {
                        'messageId': message_id,
                        'audioData': '',  # Empty data
                        'chunkNumber': final_chunk_number,
                        'totalBytes': total_bytes_sent,
                        'room': room_id,
                        'isComplete': True,  # Mark as final
                        'isRealtime': True,
                        'format': 'pcm',
                        'isFinalMarker': True  # Special flag to indicate this is just a marker
                    }, room=room_id, namespace='/ws')
            
                # Signal completion - only that streaming is done, not that playback is done
                self.socketio.emit('ai_audio_complete', {
                    'messageId': message_id,
                    'room': room_id,
                    'totalChunks': chunk_number,
                    'totalBytes': total_bytes_sent,
                    'format': 'pcm'
                }, room=room_id, namespace='/ws')
                
                # DON'T send ai_audio_done here - let frontend determine when playback is actually finished
                
                print(f"‚úÖ Streamed {chunk_number} PCM chunks ({total_bytes_sent} bytes) for message {message_id}")
            return True
            
        except Exception as e:
            print(f"Error generating streaming speech: {e}")
            # Signal error
            self.socketio.emit('ai_audio_error', {
                'messageId': message_id,
                'room': room_id,
                'error': str(e)
            }, room=room_id, namespace='/ws')
            
            # CRITICAL: Even on error, signal that audio streaming is done
            self.socketio.emit('ai_audio_done', {
                'messageId': message_id,
                'room': room_id,
                'status': 'error'
            }, room=room_id, namespace='/ws')
            
            return None

    async def send_ai_message_with_audio(self, room_id: str, content: str):
        """Send an AI message to the chat room with streaming audio - optimized for speed"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': True,  # Will have streaming audio
            'isStreaming': True  # Indicate this is a streaming response
        }
        
        # Update last response time
        if room_id in self.conversation_history:
            self.conversation_history[room_id].last_ai_response = datetime.now()
        
        # Send message immediately (don't wait for audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
        
        # Generate streaming audio in parallel (non-blocking)
        def generate_and_stream_audio():
            try:
                # Run the async audio streaming in a new event loop
                async def audio_task():
                    await self.generate_streaming_speech(content, room_id, message['id'])
                
                # Create and run new event loop for audio generation
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(audio_task())
                loop.close()
            except Exception as e:
                print(f"Error in streaming audio: {e}")
                # Fallback to simple notification that audio failed
                self.socketio.emit('ai_audio_error', {
                    'messageId': message['id'],
                    'room': room_id,
                    'error': 'Audio generation failed'
                }, room=room_id, namespace='/ws')
        
        # Start audio streaming in a separate thread
        threading.Thread(target=generate_and_stream_audio, daemon=True).start()
    
    def send_ai_message_text_only(self, room_id: str, content: str):
        """Send an AI message to the chat room without audio"""
        message = {
            'id': f"ai_{int(time.time() * 1000)}",
            'content': content,
            'username': self.agent_name,
            'userId': self.agent_id,
            'timestamp': datetime.now().isoformat(),
            'room': room_id,
            'isAI': True,
            'hasAudio': False
        }
        
        # Update last response time
        if room_id in self.conversation_history:
            self.conversation_history[room_id].last_ai_response = datetime.now()
            
        # Emit the message to the room (no audio)
        self.socketio.emit('chat_message', message, room=room_id, namespace='/ws')
    
    def send_ai_message(self, room_id: str, content: str):
        """Send an AI message to the chat room (optimized sync version)"""
        try:
            # Try to use existing event loop if available
            try:
                loop = asyncio.get_running_loop()
                asyncio.create_task(self.send_ai_message_with_audio(room_id, content))
                return
            except RuntimeError:
                # No running loop - use thread pool to avoid blocking
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.send_ai_message_with_audio(room_id, content))
                    return
        except Exception as e:
            # Fallback to simple text-only message
            self.send_ai_message_text_only(room_id, content)
        
    async def process_message(self, message_data: Dict[str, Any]):
        """Process a new message and potentially respond"""
        room_id = message_data.get('room')
        if not room_id:
            return
            
        # Add message to context
        self.add_message_to_context(message_data)
        

        # Check if we should respond
        if self.should_respond(room_id):
            print("AI will respond to the message")
            # Generate response asynchronously
            response = await self.generate_response(room_id)
            
            if response:
                # Send the response using async version
                await self.send_ai_message_with_audio(room_id, response)
                
    def process_message_sync(self, message_data: Dict[str, Any]):
        """Synchronous wrapper for processing messages"""
        try:
            # Run the async function in a new event loop
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(self.process_message(message_data))
            loop.close()
        except Exception as e:
            print(f"Error processing message in AI agent: {e}")
            # Continue processing other messages
            
    def handle_code_update(self, room_id: str, code: str, language: str = "python"):
        """Handle code updates from the editor"""
        self.update_code_context(room_id, code, language)
        
    def handle_problem_update(self, room_id: str, problem_title: str, problem_description: str):
        """Handle problem description updates"""
        self.update_problem_context(room_id, problem_title, problem_description)
        
    def join_room(self, room_id: str):
        """AI agent joins a room"""
        # Only send greeting if OpenAI client is available
        if not self.client:
            print(f"‚ö†Ô∏è  CodeBot cannot join room {room_id}: OpenAI client not initialized")
            return
            
        # Send a greeting message when joining
        greeting_messages = [
            "Hi! I'm CodeBot, your proactive AI pair programming teammate. I'll be listening and will jump in if I can help!",
        ]
        
        import random
        greeting = random.choice(greeting_messages)
        
        # Send greeting after a short delay (non-blocking)
        def send_greeting():
            time.sleep(2)  # Wait 2 seconds before greeting
            self.send_ai_message_text_only(room_id, greeting)
            
        # Start greeting in a separate thread to avoid blocking
        threading.Thread(target=send_greeting, daemon=True).start()

    def set_voice_config(self, voice: str = None, model: str = None, speed: float = None):
        """Update voice configuration for TTS"""
        if voice and voice in ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]:
            self.voice_config["voice"] = voice
        if model and model in ["tts-1", "tts-1-hd"]:
            self.voice_config["model"] = model
        if speed and 0.25 <= speed <= 4.0:
            self.voice_config["speed"] = speed

# Global AI agent instance
ai_agent = None

def init_ai_agent(socketio_instance):
    """Initialize the AI agent"""
    global ai_agent
    ai_agent = AIAgent(socketio_instance)
    return ai_agent

def get_ai_agent():
    """Get the global AI agent instance"""
    return ai_agent
